# -*- coding: utf-8 -*-
"""checking the accuracy of svm model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ShZlyaUYLCTyZFmp7mTTxzs8qEtUB7Q
"""

# --- Import necessary libraries ---
import librosa
import numpy as np
import joblib
from google.colab import files
import os

# --- Helper Function to Calculate Features ---
def calculate_feature_vector(audio, sr):
    """Calculates the same feature vector as used in training."""
    n_fft = 512
    hop_length = 256
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=n_fft, hop_length=hop_length)
    zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=hop_length)
    sc = librosa.feature.spectral_centroid(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length)
    feature_vector = np.concatenate([
        np.mean(mfccs, axis=1), np.std(mfccs, axis=1),
        [np.mean(zcr), np.std(zcr)],
        [np.mean(sc), np.std(sc)]
    ])
    return feature_vector

# --- Main Inference Script with Cleanup ---
def predict_on_new_audio():
    # --- NEW: Automatically clean up old files ---
    print("--- Preparing Session ---")
    if os.path.exists('svm_classifier.pkl'):
        os.remove('svm_classifier.pkl')
        print("Removed old svm_classifier.pkl")
    if os.path.exists('scaler.pkl'):
        os.remove('scaler.pkl')
        print("Removed old scaler.pkl")
    print("Ready for upload.")

    # 1. Load the saved model and scaler
    print("\n--- Step 1: Uploading Your Trained Model ---")
    print("Please upload your 'svm_classifier.pkl' and 'scaler.pkl' files.")
    try:
        uploaded_models = files.upload()
        if 'svm_classifier.pkl' not in uploaded_models or 'scaler.pkl' not in uploaded_models:
             print("\n❌ Error: Make sure you select and upload BOTH 'svm_classifier.pkl' and 'scaler.pkl'.")
             return
        model = joblib.load('svm_classifier.pkl')
        scaler = joblib.load('scaler.pkl')
        print("✅ Model and scaler loaded successfully.")
    except Exception as e:
        print(f"Error: Could not load model files. Details: {e}")
        return

    # 2. Upload a new audio file to test
    print("\n--- Step 2: Uploading Your Test Audio ---")
    print("Please upload the original audio file you want to test.")
    uploaded_audio = files.upload()
    if not uploaded_audio:
        print("No audio file uploaded. Exiting.")
        return

    filename = next(iter(uploaded_audio))
    with open(filename, 'wb') as f:
        f.write(uploaded_audio[filename])

    # 3. Process the new audio file and predict
    print(f"\n--- Step 3: Generating Prediction Timeline for '{filename}' ---")
    SAMPLE_RATE = 16000
    SEGMENT_MS = 500

    try:
        audio, sr = librosa.load(filename, sr=SAMPLE_RATE, mono=True)
        audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)
    except Exception as e:
        print(f"Error processing audio file: {e}")
        return

    segment_samples = int(SEGMENT_MS / 1000 * sr)
    if len(audio_trimmed) < segment_samples:
        print("\n❌ ERROR: The audio is too short to be analyzed after silence removal.")
        return

    step_size = int(segment_samples * 0.5)

    print("\nPrediction Timeline:")
    print("-" * 50)
    for start_sample in range(0, len(audio_trimmed) - segment_samples + 1, step_size):
        segment = audio_trimmed[start_sample:start_sample + segment_samples]
        feature_vector = calculate_feature_vector(segment, sr).reshape(1, -1)
        feature_vector_scaled = scaler.transform(feature_vector)
        prediction = model.predict(feature_vector_scaled)[0]
        start_time_sec = start_sample / sr
        print(f"Time: {start_time_sec:6.2f}s  ->  Predicted: {prediction}")
    print("-" * 50)

# --- Run the inference script ---
predict_on_new_audio()