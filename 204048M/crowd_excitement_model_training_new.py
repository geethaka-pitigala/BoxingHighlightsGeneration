# -*- coding: utf-8 -*-
"""crowd_excitement_model_training_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_I0BgWKcqu3R8u4ghi72fg72znupzS_o
"""

# ===================================================================
# SCRIPT 2: Train the Crowd Excitement CNN Model (Binary)
# ===================================================================
#
# Instructions:
# 1. Run this cell in a NEW Colab notebook or after the first script.
# 2. It will ask you to upload the 'excited_spectrograms.zip' and
#    'not_excited_spectrograms.zip' files.
# 3. The script will train the CNN, show performance plots, and download the
#    trained 'crowd_cnn_binary.keras' model file.
#
# ===================================================================

# --- 1. Import libraries ---
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from google.colab import files
import zipfile
import os
import matplotlib.pyplot as plt
import shutil

# --- 2. Main training script ---
def train_crowd_cnn_binary():
    # --- Clean up and create main directory ---
    if os.path.exists('dataset'): shutil.rmtree('dataset')
    if os.path.exists('temp_unzip'): shutil.rmtree('temp_unzip')
    os.makedirs('dataset/train/excited', exist_ok=True)
    os.makedirs('dataset/train/not_excited', exist_ok=True)
    os.makedirs('dataset/test/excited', exist_ok=True)
    os.makedirs('dataset/test/not_excited', exist_ok=True)

    # --- Upload and Unzip Spectrograms ---
    for category in ['excited', 'not_excited']:
        print(f"--- Please upload your '{category}_spectrograms.zip' file ---")
        uploaded = files.upload()
        if not uploaded:
            print(f"No file uploaded for '{category}'. Skipping.")
            continue

        zip_path = next(iter(uploaded))
        unzip_dir = f'temp_unzip/{category}'
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(unzip_dir)
        print(f"{category} spectrograms unzipped successfully.")

        # --- FIX: Intelligently find and move files ---
        for split in ['train', 'test']:
            source_dir = os.path.join(unzip_dir, split, category)
            # Handle cases where the zip might have an extra top-level folder
            if not os.path.exists(source_dir):
                # Search one level deeper
                for root, dirs, _ in os.walk(unzip_dir):
                    if split in dirs:
                        potential_source = os.path.join(root, split, category)
                        if os.path.exists(potential_source):
                            source_dir = potential_source
                            break

            destination_dir = f'dataset/{split}/{category}'
            if os.path.exists(source_dir):
                for file_name in os.listdir(source_dir):
                    shutil.move(os.path.join(source_dir, file_name), destination_dir)
                print(f"Moved {split} files for '{category}' to the correct location.")
            else:
                print(f"⚠️ Warning: Could not find source directory: {source_dir}")

    # --- Load Data ---
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32

    train_dir = 'dataset/train'
    test_dir = 'dataset/test'

    if not os.path.isdir(train_dir) or not os.listdir(train_dir):
        print(f"❌ ERROR: The training directory '{train_dir}' is empty or does not exist.")
        return

    train_dataset = tf.keras.utils.image_dataset_from_directory(
        train_dir,
        labels='inferred',
        label_mode='categorical',
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE
    )

    test_dataset = tf.keras.utils.image_dataset_from_directory(
        test_dir,
        labels='inferred',
        label_mode='categorical',
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE
    )

    class_names = train_dataset.class_names
    print("Found classes:", class_names)

    # --- Build the CNN Model ---
    model = keras.Sequential([
        layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
        layers.Rescaling(1./255),
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(len(class_names), activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    model.summary()

    # --- Train the Model ---
    print("\n--- Starting Model Training ---")
    history = model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=15
    )

    # --- Plot Results ---
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs_range = range(15)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

    # --- Save and Download the Model ---
    print("\nSaving the trained model...")
    model.save('crowd_cnn_binary.keras')
    print("✅ Model saved. Downloading now...")
    files.download('crowd_cnn_binary.keras')

# --- Run the training process ---
train_crowd_cnn_binary() # Remember to uncomment this line to run the training script