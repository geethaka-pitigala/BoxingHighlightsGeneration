# -*- coding: utf-8 -*-
"""final pipeline with time range.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RAtgiGQzYr6qIPM5OowlI4k7Y6PCqY27
"""

# ===================================================================
# FINAL INTEGRATED PIPELINE
# ===================================================================
#
# Instructions:
# 1. Run this cell in a NEW Colab notebook.
# 2. It will guide you to upload all 4 of your trained models.
# 3. Then, it will ask for the original mixed audio file to analyze.
# 4. The script will run the full analysis and print the final lists of
#    long-duration exciting moments.
#
# ===================================================================

# --- 1. Install and import necessary libraries ---
print("Installing libraries...")
!pip install librosa pandas openpyxl -q
!pip install tensorflow -q

import librosa
import librosa.display
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from google.colab import files
import os
import shutil
import matplotlib.pyplot as plt

# --- 2. Define All Helper Functions ---

# --- Helpers for Triage Classifier (SVM #1) ---
def calculate_segment_features(audio, sr):
    n_fft, hop_length = 512, 256
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=n_fft, hop_length=hop_length)
    zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=hop_length)
    sc = librosa.feature.spectral_centroid(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length)
    return np.concatenate([np.mean(mfccs, axis=1), np.std(mfccs, axis=1), [np.mean(zcr), np.std(zcr)], [np.mean(sc), np.std(sc)]])

# --- Helpers for Separation Algorithm (SVM #2) ---
def separate_mixed_chunk(audio_chunk, sr, model, scaler):
    N_FFT, HOP_LENGTH = 1024, 512
    stft_mixed = librosa.stft(audio_chunk, n_fft=N_FFT, hop_length=HOP_LENGTH)
    mag_mixed, phase_mixed = np.abs(stft_mixed), np.angle(stft_mixed)
    speech_mask = np.zeros_like(mag_mixed)
    speech_class_index = np.where(model.classes_ == 'commentator_only')[0][0]
    freqs = librosa.fft_frequencies(sr=sr, n_fft=N_FFT)

    for t in range(mag_mixed.shape[1]):
        frame_features = extract_bin_features_for_frame(mag_mixed[:, t], freqs)
        if len(frame_features) == 0: continue
        frame_features_scaled = scaler.transform(frame_features)
        frame_probs = model.predict_proba(frame_features_scaled)
        speech_mask[:, t] = frame_probs[:, speech_class_index]

    crowd_mask = 1 - speech_mask
    mag_speech = mag_mixed * speech_mask
    mag_crowd = mag_mixed * crowd_mask
    audio_speech = librosa.istft(mag_speech * np.exp(1j * phase_mixed), hop_length=HOP_LENGTH, length=len(audio_chunk))
    audio_crowd = librosa.istft(mag_crowd * np.exp(1j * phase_mixed), hop_length=HOP_LENGTH, length=len(audio_chunk))
    return audio_speech, audio_crowd

def extract_bin_features_for_frame(magnitude_frame, freqs):
    features = []
    for f in range(len(magnitude_frame)):
        mag_val = magnitude_frame[f]
        v_neighbors = magnitude_frame[max(0, f-1):min(len(magnitude_frame), f+2)]
        features.append([mag_val, freqs[f], np.mean(v_neighbors), np.mean(v_neighbors)])
    return np.array(features)

# --- CORRECTED Helper for Excitement Analyzers (CNNs) ---
def create_spectrogram_image(audio_chunk, sr, target_height, target_width, colormap='viridis'):
    """
    Creates a Mel Spectrogram image and resizes it to the specific dimensions
    required by the pre-trained CNN model.
    """
    S = librosa.feature.melspectrogram(y=audio_chunk, sr=sr, n_mels=target_height, fmax=8000)
    S_DB = librosa.power_to_db(S, ref=np.max)

    # Create the image using matplotlib to apply the colormap
    fig = plt.figure(figsize=(target_width/100, target_height/100), dpi=100)
    ax = plt.Axes(fig, [0., 0., 1., 1.])
    ax.set_axis_off()
    fig.add_axes(ax)
    librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap=colormap)

    # --- FIX: Use the modern method to convert plot to numpy array ---
    fig.canvas.draw()
    buf = fig.canvas.buffer_rgba()
    img_array = np.asarray(buf)
    # Get rid of the alpha channel (4th channel)
    img = img_array[:,:,:3]
    plt.close(fig)

    # Normalize and prepare for the model
    img_tensor = tf.convert_to_tensor(img, dtype=tf.float32) / 255.0
    img_resized = tf.image.resize(img_tensor, [target_height, target_width])

    return np.expand_dims(img_resized, axis=0)


# --- Helper for Grouping Timestamps ---
def group_consecutive_timestamps(timeline, min_duration=1.0):
    if not timeline: return []
    grouped = []
    start_event = timeline[0]
    end_event = timeline[0]
    for i in range(1, len(timeline)):
        if timeline[i] == end_event + 1:
            end_event = timeline[i]
        else:
            duration = (end_event + 1) - start_event
            if duration >= min_duration:
                grouped.append(f"{start_event:.2f}s - {end_event + 1:.2f}s")
            start_event = timeline[i]
            end_event = timeline[i]
    duration = (end_event + 1) - start_event
    if duration >= min_duration:
        grouped.append(f"{start_event:.2f}s - {end_event + 1:.2f}s")
    return grouped

# --- 3. The Main Pipeline Function ---
def run_full_pipeline():
    # --- Part 1: Load All Trained Models ---
    print("--- Step 1: Uploading All Your Trained Models ---")
    for f in ['svm_classifier.pkl', 'scaler.pkl', 'bin_classifier.pkl', 'bin_scaler.pkl', 'commentator_cnn.h5', 'crowd_cnn.keras']:
        if os.path.exists(f): os.remove(f)

    print("Please upload your two SVM models and their scalers (.pkl files)...")
    try:
        files.upload()
        segment_classifier = joblib.load('svm_classifier.pkl')
        segment_scaler = joblib.load('scaler.pkl')
        bin_classifier = joblib.load('bin_classifier.pkl')
        bin_scaler = joblib.load('bin_scaler.pkl')
        print("✅ SVM models loaded successfully.")
    except Exception as e:
        print(f"❌ Error loading SVM models: {e}. Please ensure all 4 .pkl files are uploaded.")
        return

    print("\nPlease upload your two CNN excitement models (.h5 and .keras files)...")
    try:
        files.upload()
        commentator_cnn = tf.keras.models.load_model('commentator_cnn.h5')
        crowd_cnn = tf.keras.models.load_model('crowd_cnn.keras')
        print("✅ CNN models loaded successfully.")
    except Exception as e:
        print(f"❌ Error loading CNN models: {e}. Please ensure both files are uploaded.")
        return

    # --- NEW: Automatically get the required input shape from the CNNs ---
    comm_input_shape = commentator_cnn.input_shape
    crowd_input_shape = crowd_cnn.input_shape
    COMM_IMG_HEIGHT, COMM_IMG_WIDTH = comm_input_shape[1], comm_input_shape[2]
    CROWD_IMG_HEIGHT, CROWD_IMG_WIDTH = crowd_input_shape[1], crowd_input_shape[2]
    print(f"Commentator CNN expects input shape: ({COMM_IMG_HEIGHT}, {COMM_IMG_WIDTH})")
    print(f"Crowd CNN expects input shape: ({CROWD_IMG_HEIGHT}, {CROWD_IMG_WIDTH})")

    # --- Part 2: Get Input Audio ---
    print("\n--- Step 2: Uploading Original Audio ---")
    print("Please upload the original mixed audio file you want to analyze.")
    uploaded_audio = files.upload()
    if not uploaded_audio:
        print("No audio file uploaded. Exiting.")
        return
    filename = next(iter(uploaded_audio))
    with open(filename, 'wb') as f: f.write(uploaded_audio[filename])

    # --- Part 3: Run the Pipeline ---
    print(f"\n--- Step 3: Running Full Analysis on '{filename}' ---")
    SAMPLE_RATE = 22050

    audio, sr = librosa.load(filename, sr=SAMPLE_RATE, mono=True)
    audio, _ = librosa.effects.trim(audio, top_db=20)

    # --- Triage Classification ---
    print("-> Performing Triage Classification...")
    triage_segment_samples = 1 * sr
    triage_labels = []
    for start in range(0, len(audio) - triage_segment_samples + 1, triage_segment_samples):
        chunk = audio[start:start+triage_segment_samples]
        feature_vec = calculate_segment_features(chunk, sr).reshape(1, -1)
        feature_vec_scaled = segment_scaler.transform(feature_vec)
        triage_labels.append(segment_classifier.predict(feature_vec_scaled)[0])

    # --- Separation and Reconstruction ---
    print("-> Separating Mixed Segments and Reconstructing Tracks...")
    final_commentator_track = []
    final_crowd_track = []
    for i, label in enumerate(triage_labels):
        start = i * triage_segment_samples
        end = start + triage_segment_samples
        chunk = audio[start:end]

        if label == 'mixed':
            s_comm, s_crowd = separate_mixed_chunk(chunk, sr, bin_classifier, bin_scaler)
        elif label == 'commentator_only':
            s_comm, s_crowd = chunk, np.zeros_like(chunk)
        else: # crowd_only
            s_comm, s_crowd = np.zeros_like(chunk), chunk

        final_commentator_track.append(s_comm)
        final_crowd_track.append(s_crowd)

    final_commentator_track = np.concatenate(final_commentator_track)
    final_crowd_track = np.concatenate(final_crowd_track)

    # --- Excitement Analysis ---
    print("-> Analyzing Excitement Levels...")
    commentator_excitement_timeline = []
    crowd_excitement_timeline = []
    commentator_labels = ['excited', 'not_excited']
    crowd_labels = ['excited', 'not_excited']

    excite_segment_samples = 1 * sr
    for i in range(len(triage_labels)):
        start = i * excite_segment_samples
        end = start + excite_segment_samples

        comm_chunk = final_commentator_track[start:end]
        comm_img = create_spectrogram_image(comm_chunk, sr, COMM_IMG_HEIGHT, COMM_IMG_WIDTH, colormap='magma') # Purplish
        comm_pred = commentator_cnn.predict(comm_img, verbose=0)
        comm_label = commentator_labels[np.argmax(comm_pred)]
        commentator_excitement_timeline.append(1 if comm_label == 'excited' else 0)

        crowd_chunk = final_crowd_track[start:end]
        crowd_img = create_spectrogram_image(crowd_chunk, sr, CROWD_IMG_HEIGHT, CROWD_IMG_WIDTH, colormap='jet') # Yellow/Red
        crowd_pred = crowd_cnn.predict(crowd_img, verbose=0)
        crowd_label = crowd_labels[np.argmax(crowd_pred)]
        crowd_excitement_timeline.append(1 if crowd_label == 'excited' else 0)

    # --- Group Timestamps and Finalize Output ---
    print("-> Grouping Timestamps and Generating Final Report...")
    only_crowd_excited_ts = [i for i, (c, s) in enumerate(zip(crowd_excitement_timeline, commentator_excitement_timeline)) if c == 1 and s == 0]
    only_commentator_excited_ts = [i for i, (c, s) in enumerate(zip(crowd_excitement_timeline, commentator_excitement_timeline)) if c == 0 and s == 1]
    both_excited_ts = [i for i, (c, s) in enumerate(zip(crowd_excitement_timeline, commentator_excitement_timeline)) if c == 1 and s == 1]

    # --- Display the final results ---
    print("\n\n--- FINAL EVALUATION RESULTS ---")
    print("\n[ 1. TIMESTAMPS WHERE ONLY THE CROWD WAS EXCITED ]")
    print(group_consecutive_timestamps(only_crowd_excited_ts))

    print("\n[ 2. TIMESTAMPS WHERE ONLY THE COMMENTATOR WAS EXCITED ]")
    print(group_consecutive_timestamps(only_commentator_excited_ts))

    print("\n[ 3. TIMESTAMPS WHERE BOTH WERE EXCITED (KEY MOMENTS) ]")
    print(group_consecutive_timestamps(both_excited_ts))

# --- Run the entire pipeline ---
run_full_pipeline()