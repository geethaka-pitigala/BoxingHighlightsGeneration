# -*- coding: utf-8 -*-
"""bin_classifier_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12SajdSF1xcGmvMhiw5DA5_eC8dWbBetJ
"""

# --- Import libraries ---
import librosa
import numpy as np
import pandas as pd
from google.colab import files
import os
import shutil

# --- Helper Function (same as before) ---
def extract_bin_features(audio, sr, magnitude_threshold=0.01):
    N_FFT = 1024
    HOP_LENGTH = 512
    stft_result = librosa.stft(audio, n_fft=N_FFT, hop_length=HOP_LENGTH)
    magnitude = np.abs(stft_result)
    freqs = librosa.fft_frequencies(sr=sr, n_fft=N_FFT)
    bin_features = []
    num_frames, num_freq_bins = magnitude.shape[1], magnitude.shape[0]
    for t in range(num_frames):
        for f in range(num_freq_bins):
            mag_val = magnitude[f, t]
            if mag_val > magnitude_threshold:
                freq_val = freqs[f]
                h_neighbors = magnitude[f, max(0, t-1):min(num_frames, t+2)]
                v_neighbors = magnitude[max(0, f-1):min(num_freq_bins, f+2), t]
                bin_features.append([mag_val, freq_val, np.mean(h_neighbors), np.mean(v_neighbors)])
    return bin_features

# --- Main script to create separate datasets ---
def create_separate_bin_datasets():
    SAMPLE_RATE = 16000

    # --- Process each category one by one ---
    for category in ['commentator_only', 'crowd_only']:
        # This list will only hold features for the current category
        category_features = []

        # Clean up temp folder for each run
        if os.path.exists('temp_audio'): shutil.rmtree('temp_audio')
        os.makedirs('temp_audio', exist_ok=True)

        print(f"--- Please upload your CLEAN '{category}' audio files ---")
        uploaded = files.upload()
        if not uploaded:
            print(f"No files uploaded for {category}. Skipping.")
            continue

        print(f"Processing {len(uploaded)} file(s) for '{category}'...")
        for filename, content in uploaded.items():
            temp_path = os.path.join('temp_audio', filename)
            with open(temp_path, 'wb') as f:
                f.write(content)

            try:
                audio, sr = librosa.load(temp_path, sr=SAMPLE_RATE, mono=True)
                audio, _ = librosa.effects.trim(audio, top_db=20)
                features = extract_bin_features(audio, sr)
                for feature_vector in features:
                    feature_vector.append(category)
                category_features.extend(features)
                print(f"-> Extracted {len(features)} feature points from {filename}")
            except Exception as e:
                print(f"Could not process {filename}: {e}")

        # --- Save and download the dataset for this category immediately ---
        if category_features:
            print(f"\nCreating dataset file for '{category}'...")
            df = pd.DataFrame(category_features, columns=['magnitude', 'frequency', 'h_neighbor_mean', 'v_neighbor_mean', 'class_label'])
            csv_filename = f'{category}_features.csv'
            df.to_csv(csv_filename, index=False)
            print(f"âœ… Dataset created: '{csv_filename}'")
            print(f"Downloading '{csv_filename}' to your computer...")
            files.download(csv_filename)
        else:
            print(f"\nNo features were extracted for {category}.")

        # Clean up temp files before next category
        shutil.rmtree('temp_audio')

# --- Run the full process ---
create_separate_bin_datasets()