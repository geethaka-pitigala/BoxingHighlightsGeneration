# -*- coding: utf-8 -*-
"""dataSetCreationOlympic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12CKy2oa07O73Irl1v4TyhgGfw4TpR8u8
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Install necessary libraries
!{sys.executable} -m pip install ultralytics opencv-python pandas tqdm -q

print("✅ Setup complete.")

import os
import glob
import json
import cv2
import pandas as pd
from collections import defaultdict
from ultralytics import YOLO

# --- 1. UPDATE YOUR FILE PATHS HERE ---
# Path to the main folder containing all your 'task_kam...' subfolders
DATASET_ROOT_PATH = '/content/drive/MyDrive/University/Research/Olympic Boxer dataset Adaptation/Visualize/Classification Model train/dataset'
OUTPUT_CSV_PATH = '/content/drive/MyDrive/University/Research/Olympic Boxer dataset Adaptation/Visualize/Classification Model train/full_feature_dataset3_p2.csv'
PLAYER_MODEL_PATH = '/content/drive/MyDrive/University/Research/Olympic Boxer dataset Adaptation/Visualize/Classification Model train/olympicYolo.pt'

# --- 2. LOAD MODELS ---
try:
    player_model = YOLO(PLAYER_MODEL_PATH)
    print("✅ Custom player detection model loaded.")
except Exception as e:
    print(f"❌ Error loading custom model: {e}")
    sys.exit()

pose_model = YOLO('yolov8n-pose.pt')
print("✅ YOLOv8-Pose model loaded.")

# --- 3. DISCOVER ALL VIDEO/ANNOTATION PAIRS ---
def find_data_pairs(root_path):
    pairs = []
    # Use glob to find all annotations.json files recursively
    annotation_files = glob.glob(os.path.join(root_path, '**', 'annotations.json'), recursive=True)
    for ann_path in annotation_files:
        # Construct the corresponding video path
        video_path = os.path.join(os.path.dirname(ann_path), 'data', '*.mp4')
        video_files = glob.glob(video_path)
        if video_files:
            pairs.append({'annotation': ann_path, 'video': video_files[0]})
    return pairs

data_pairs = find_data_pairs(DATASET_ROOT_PATH)
print(f"✅ Discovered {len(data_pairs)} video/annotation pairs.")

import numpy as np

# --- Keypoint Constants ---
R_WRIST, L_WRIST, R_SHOULDER, L_SHOULDER, R_ELBOW, L_ELBOW = 10, 9, 6, 5, 8, 7

def calculate_distance(p1, p2):
    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)

def calculate_angle(p1, p2, p3):
    a, b, c = np.array(p1), np.array(p2), np.array(p3)
    ba, bc = a - b, c - b
    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
    return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))

def get_meters_per_pixel(kpts):
    AVG_SHOULDER_WIDTH_METERS = 0.45
    try:
        l_shoulder, r_shoulder = kpts[L_SHOULDER], kpts[R_SHOULDER]
        if (len(l_shoulder) == 3 and l_shoulder[2] == 0) or (len(r_shoulder) == 3 and r_shoulder[2] == 0): return None
        pixel_dist = calculate_distance(l_shoulder, r_shoulder)
        return AVG_SHOULDER_WIDTH_METERS / pixel_dist if pixel_dist > 0 else None
    except (IndexError, ZeroDivisionError):
        return None

# --- NEW: Data Sanitization Function ---
def is_features_valid(features):
    """Checks if the calculated features are within a realistic range."""
    if features['peak_velocity'] > 45: return False # ~100 mph is impossible
    if features['peak_acceleration'] > 10000: return False # Extreme acceleration is likely an error
    return True

from collections import deque
from tqdm import tqdm # For a nice progress bar
import random

# --- Analysis Parameters ---
WINDOW_SIZE = 12
NEGATIVE_SAMPLES_RATIO = 3 # Generate 3 negative samples for every 1 positive sample

# --- Master list to hold all results ---
all_results = []

# --- Loop through each video/annotation pair ---
for pair in tqdm(data_pairs, desc="Processing Dataset"):
    VIDEO_PATH = pair['video']
    ANNOTATIONS_PATH = pair['annotation']

    with open(ANNOTATIONS_PATH, 'r') as f:
        annotations_data = json.load(f)
    event_tracks = annotations_data[0]['tracks']

    cap = cv2.VideoCapture(VIDEO_PATH)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # --- 1. Generate Positive Samples ---
    positive_event_frames = set()
    for event in event_tracks:
        punch_start_frame = event['frame']
        punch_box = event['shapes'][0]['points']
        punch_center_x = (punch_box[0] + punch_box[2]) / 2

        cap.set(cv2.CAP_PROP_POS_FRAMES, punch_start_frame)
        ret, frame = cap.read()
        if not ret: continue

        player_results = player_model(frame, verbose=False, conf=0.2)
        detected_boxes = player_results[0].boxes.xyxy.cpu().numpy()

        closest_player_box = None
        min_dist = float('inf')
        for box in detected_boxes:
            dist = abs(punch_center_x - ((box[0] + box[2]) / 2))
            if dist < min_dist:
                min_dist, closest_player_box = dist, box

        if closest_player_box is None: continue

        event_keypoints, event_frames = [], []
        for shape in event['shapes']:
            if shape.get('outside', False): break
            frame_num = shape['frame']
            positive_event_frames.add(frame_num)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            ret, current_frame = cap.read()
            if not ret: continue

            x1, y1, x2, y2 = map(int, closest_player_box)
            cropped_player = current_frame[y1:y2, x1:x2]
            if cropped_player.size == 0: continue

            pose_results = pose_model(cropped_player, verbose=False)
            if len(pose_results) > 0 and pose_results[0].keypoints and len(pose_results[0].keypoints.data) > 0:
                kpts = pose_results[0].keypoints.data[0].cpu().numpy()
                kpts[:, 0] += x1; kpts[:, 1] += y1
                event_keypoints.append(kpts)
                event_frames.append(frame_num)

        if len(event_keypoints) < 2: continue
        mpp = get_meters_per_pixel(event_keypoints[0])
        if mpp is None: continue

        label = event['label']
        hand = 'left' if 'lewą' in label else 'right'
        wrist_idx, elbow_idx, shoulder_idx = (L_WRIST, L_ELBOW, L_SHOULDER) if hand == 'left' else (R_WRIST, R_ELBOW, R_SHOULDER)
        wrist_positions = [kpts[wrist_idx][:2] for kpts in event_keypoints]
        velocities_mps = [(calculate_distance(wrist_positions[i], wrist_positions[i-1]) * mpp) * fps for i in range(1, len(wrist_positions))]
        if not velocities_mps: continue

        # --- THIS IS THE CORRECTED LINE ---
        accelerations_mps2 = [(velocities_mps[i] - velocities_mps[i-1]) * fps for i in range(1, len(velocities_mps))]

        if not accelerations_mps2: accelerations_mps2 = [0]
        angles = [calculate_angle(kpts[shoulder_idx][:2], kpts[elbow_idx][:2], kpts[wrist_idx][:2]) for kpts in event_keypoints]
        peak_velocity_idx = np.argmax(velocities_mps)
        features = {
            'label': label, 'peak_velocity': max(velocities_mps), 'mean_velocity': np.mean(velocities_mps),
            'std_velocity': np.std(velocities_mps), 'peak_acceleration': max(accelerations_mps2),
            'mean_acceleration': np.mean(accelerations_mps2),
            'angle_at_peak': angles[peak_velocity_idx + 1] if peak_velocity_idx + 1 < len(angles) else angles[-1],
            'mean_angle': np.mean(angles),
            'total_displacement': calculate_distance(wrist_positions[0], wrist_positions[-1]) * mpp,
        }

        if is_features_valid(features):
            all_results.append(features)

    # --- 2. Generate Negative Samples ---
    num_positive_events = len(event_tracks)
    num_negative_to_generate = num_positive_events * NEGATIVE_SAMPLES_RATIO
    negative_samples_generated = 0

    all_video_frames = set(range(total_frames))
    negative_candidate_frames = list(all_video_frames - positive_event_frames)
    random.shuffle(negative_candidate_frames)

    for frame_num in negative_candidate_frames:
        if negative_samples_generated >= num_negative_to_generate: break

        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
        ret, frame = cap.read()
        if not ret: continue

        player_results = player_model(frame, verbose=False, conf=0.7)

        for box in player_results[0].boxes.xyxy.cpu().numpy():
            if negative_samples_generated >= num_negative_to_generate: break

            window_keypoints = []
            for i in range(WINDOW_SIZE):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num + i)
                ret_win, frame_win = cap.read()
                if not ret_win: break

                x1, y1, x2, y2 = map(int, box)
                cropped_player = frame_win[y1:y2, x1:x2]
                if cropped_player.size == 0: continue

                pose_results = pose_model(cropped_player, verbose=False)
                if len(pose_results) > 0 and pose_results[0].keypoints and len(pose_results[0].keypoints.data) > 0:
                    kpts = pose_results[0].keypoints.data[0].cpu().numpy()
                    kpts[:, 0] += x1; kpts[:, 1] += y1
                    window_keypoints.append(kpts)

            if len(window_keypoints) < WINDOW_SIZE: continue
            mpp = get_meters_per_pixel(window_keypoints[0])
            if mpp is None: continue

            for side in ['left', 'right']:
                if negative_samples_generated >= num_negative_to_generate: break
                try:
                    wrist_idx = L_WRIST if side == 'left' else R_WRIST
                    elbow_idx = L_ELBOW if side == 'left' else R_ELBOW
                    shoulder_idx = L_SHOULDER if side == 'left' else R_SHOULDER

                    wrist_positions = [kpts[wrist_idx][:2] for kpts in window_keypoints]
                    velocities_mps = [(calculate_distance(wrist_positions[i], wrist_positions[i-1]) * mpp) * fps for i in range(1, len(wrist_positions))]
                    if not velocities_mps: continue

                    # --- THIS IS THE SECOND CORRECTED LINE ---
                    accelerations_mps2 = [(velocities_mps[i] - velocities_mps[i-1]) * fps for i in range(1, len(velocities_mps))]

                    if not accelerations_mps2: accelerations_mps2 = [0]
                    angles = [calculate_angle(kpts[shoulder_idx][:2], kpts[elbow_idx][:2], kpts[wrist_idx][:2]) for kpts in window_keypoints]
                    peak_velocity_idx = np.argmax(velocities_mps)

                    features = {
                        'label': 'Non-Event', 'peak_velocity': max(velocities_mps), 'mean_velocity': np.mean(velocities_mps),
                        'std_velocity': np.std(velocities_mps), 'peak_acceleration': max(accelerations_mps2),
                        'mean_acceleration': np.mean(accelerations_mps2),
                        'angle_at_peak': angles[peak_velocity_idx + 1] if peak_velocity_idx + 1 < len(angles) else angles[-1],
                        'mean_angle': np.mean(angles),
                        'total_displacement': calculate_distance(wrist_positions[0], wrist_positions[-1]) * mpp,
                    }

                    if is_features_valid(features):
                        all_results.append(features)
                        negative_samples_generated += 1
                except IndexError:
                    continue
    cap.release()

print("\n✅ All videos processed.")

import pandas as pd

# 1. Convert the list of dictionaries into a DataFrame
final_df = pd.DataFrame(all_results)

# 2. Save the DataFrame to the CSV file at your specified path
#    The 'index=False' argument prevents Pandas from writing an extra column for the row numbers.
final_df.to_csv(OUTPUT_CSV_PATH, index=False)

print(f"✅ Success! Your dataset has been saved to: {OUTPUT_CSV_PATH}")