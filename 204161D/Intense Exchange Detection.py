# -*- coding: utf-8 -*-
"""Integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9nLMR4UCDK5K6K5l8GiF8sThkQ6jhnW
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Install necessary libraries
!{sys.executable} -m pip install ultralytics opencv-python pandas joblib scikit-learn ffmpeg-python -q

print("Setup complete.")

import cv2
from ultralytics import YOLO
import joblib
import os
import numpy as np
from collections import deque
import pandas as pd
from tqdm import tqdm
import json

# File Paths

INPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/Integration/input/match2round2.mp4'
# Folder to save the final highlight clips
OUTPUT_CLIPS_FOLDER = '/content/drive/MyDrive/University/Research/Integration/output/highlight_clips/match2round2/'
# Path to your trained commercial boxer detection model
PLAYER_MODEL_PATH = '/content/drive/MyDrive/University/Research/Integration/playerIdentificationModel2.pt'
# Path to your saved binary classification model
CLASSIFIER_MODEL_PATH = '/content/drive/MyDrive/University/Research/Integration/punch_classifier_model_binary_v2.joblib'
# Path to save the list of detected punch frames
PUNCH_TIMESTAMPS_PATH = '/content/drive/MyDrive/University/Research/Integration/output/DetectedPunches/match2round2/detected_punches_match2round2.json'

# --- 2. LOAD MODELS ---
player_model = YOLO(PLAYER_MODEL_PATH)
pose_model = YOLO('yolov8n-pose.pt')
classifier_model = joblib.load(CLASSIFIER_MODEL_PATH)
print("All models loaded.")

"""# Counting Punches"""

# --- Keypoint Constants ---
R_WRIST, L_WRIST, R_SHOULDER, L_SHOULDER, R_ELBOW, L_ELBOW = 10, 9, 6, 5, 8, 7

def calculate_distance(p1, p2):
    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)

def calculate_angle(p1, p2, p3):
    a, b, c = np.array(p1), np.array(p2), np.array(p3)
    ba, bc = a - b, c - b
    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
    return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))

def get_meters_per_pixel(kpts):
    AVG_SHOULDER_WIDTH_METERS = 0.45
    try:
        l_shoulder, r_shoulder = kpts[L_SHOULDER], kpts[R_SHOULDER]
        pixel_dist = calculate_distance(l_shoulder, r_shoulder)
        return AVG_SHOULDER_WIDTH_METERS / pixel_dist if pixel_dist > 0 else None
    except (IndexError, ZeroDivisionError):
        return None

punch_detections = []
cap = cv2.VideoCapture(INPUT_VIDEO_PATH)

if not cap.isOpened():
    print(f"FATAL ERROR: Could not open video file.")
else:
    print("Video file opened. Starting punch detection...")
    fps = cap.get(cv2.CAP_PROP_FPS)

    boxer_tracker, next_boxer_id, history, cooldowns = {}, 0, {}, {}
    WINDOW_SIZE = 12
    PUNCH_COOLDOWN = int(fps / 2)
    frame_idx = 0
    pbar = tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), disable=True)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        frame_idx += 1
        pbar.update(1)

        new_tracker = {}
        for boxer_id in list(cooldowns.keys()):
            if cooldowns[boxer_id] > 0:
                cooldowns[boxer_id] -= 1

        player_results = player_model(frame, verbose=False, conf=0.4)
        detected_boxes = player_results[0].boxes.xyxy.cpu().numpy()

        current_detections, assigned_ids = {i: None for i in range(len(detected_boxes))}, set()
        for boxer_id, last_center in boxer_tracker.items():
            min_dist, best_match_idx = 150, -1
            for i, box in enumerate(detected_boxes):
                if i in assigned_ids: continue
                center_x = (box[0] + box[2]) / 2
                dist = abs(center_x - last_center)
                if dist < min_dist: min_dist, best_match_idx = dist, i
            if best_match_idx != -1:
                current_detections[best_match_idx], assigned_ids = boxer_id, assigned_ids | {best_match_idx}

        for i in range(len(detected_boxes)):
            if current_detections[i] is None:
                new_id = next_boxer_id
                current_detections[i] = new_id
                history[new_id] = {'left': deque(maxlen=WINDOW_SIZE), 'right': deque(maxlen=WINDOW_SIZE)}
                cooldowns[new_id], next_boxer_id = 0, next_boxer_id + 1

        for i, boxer_id in current_detections.items():
            box = detected_boxes[i]
            x1, y1, x2, y2 = map(int, box)
            new_tracker[boxer_id] = (x1 + x2) / 2
            cropped_boxer = frame[y1:y2, x1:x2]
            if cropped_boxer.size == 0: continue

            pose_results = pose_model(cropped_boxer, verbose=False)

            if len(pose_results) > 0 and pose_results[0].keypoints and len(pose_results[0].keypoints.data) > 0:
                kpts = pose_results[0].keypoints.data[0].cpu().numpy()
                kpts[:, 0], kpts[:, 1] = kpts[:, 0] + x1, kpts[:, 1] + y1
                mpp = get_meters_per_pixel(kpts)
                if mpp is None: continue

                for side in ['left', 'right']:
                    try:
                        history[boxer_id][side].append({'wrist': kpts[L_WRIST if side == 'left' else R_WRIST], 'elbow': kpts[L_ELBOW if side == 'left' else R_ELBOW], 'shoulder': kpts[L_SHOULDER if side == 'left' else R_SHOULDER]})
                    except IndexError: continue

                    if len(history[boxer_id][side]) == WINDOW_SIZE and cooldowns.get(boxer_id, 0) == 0:
                        wrist_positions = [h['wrist'][:2] for h in history[boxer_id][side]]
                        velocities = [(calculate_distance(wrist_positions[i], wrist_positions[i-1]) * mpp) * fps for i in range(1, len(wrist_positions))]
                        if not velocities: continue

                        accelerations = [(velocities[i] - velocities[i-1]) * fps for i in range(1, len(velocities))]
                        if not accelerations: accelerations = [0]
                        angles = [calculate_angle(h['shoulder'][:2], h['elbow'][:2], h['wrist'][:2]) for h in history[boxer_id][side]]
                        peak_velocity_idx = np.argmax(velocities)

                        features = {'peak_velocity': max(velocities), 'mean_velocity': np.mean(velocities),
                                    'std_velocity': np.std(velocities), 'peak_acceleration': max(accelerations),
                                    'mean_acceleration': np.mean(accelerations),
                                    'angle_at_peak': angles[peak_velocity_idx + 1] if peak_velocity_idx + 1 < len(angles) else angles[-1],
                                    'mean_angle': np.mean(angles),
                                    'total_displacement': calculate_distance(wrist_positions[0], wrist_positions[-1]) * mpp}

                        features_df = pd.DataFrame([features])
                        prediction = classifier_model.predict(features_df)[0]

                        if prediction == 1:
                            punch_detections.append(frame_idx)
                            cooldowns[boxer_id] = PUNCH_COOLDOWN
                            history[boxer_id][side].clear()

        boxer_tracker = new_tracker

    pbar.close()
    cap.release()

# Get the directory name from the full file path
output_dir = os.path.dirname(PUNCH_TIMESTAMPS_PATH)

# Create the directory if it doesn't already exist
os.makedirs(output_dir, exist_ok=True)

# Now, save the file safely
with open(PUNCH_TIMESTAMPS_PATH, 'w') as f:
  json.dump(punch_detections, f)

print(f"Detection complete! Found {len(punch_detections)} punches and saved timestamps to {PUNCH_TIMESTAMPS_PATH}")

"""# Binning & Top N"""

import matplotlib.pyplot as plt
import seaborn as sns
import json
import cv2
import numpy as np
import pandas as pd
import os
import ffmpeg

# --- LOAD DATA AND VIDEO INFO ---
print("Loading data...")
try:
    with open(PUNCH_TIMESTAMPS_PATH, 'r') as f:
        punch_detections = json.load(f)
    print(f"Loaded {len(punch_detections)} punch timestamps.")
except FileNotFoundError:
    raise FileNotFoundError(f"Timestamp file not found at {PUNCH_TIMESTAMPS_PATH}")

# This part only reads the video's metadata, not the frames.
cap = cv2.VideoCapture(INPUT_VIDEO_PATH)
if not cap.isOpened():
    raise IOError(f"Could not open video file at {INPUT_VIDEO_PATH}")

fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
video_duration_seconds = total_frames / fps
cap.release()
print(f"Video Info: {fps:.2f} FPS, Duration: {video_duration_seconds:.2f} seconds")

os.makedirs(OUTPUT_CLIPS_FOLDER, exist_ok=True)

# --- HIGHLIGHT GENERATION LOGIC ---
def seconds_to_timestamp(seconds):
    return pd.to_datetime(seconds, unit='s').strftime('%H:%M:%S.%f')[:-3]

# Parameters
BIN_WIDTH_SECONDS = 10.0
TOP_N_BINS = int((video_duration_seconds / BIN_WIDTH_SECONDS) * 0.3)
PADDING_SECONDS = 3.0

# --- CONVERT FRAMES TO SECONDS ---
punch_times_seconds = [frame / fps for frame in punch_detections]

# --- VISUALIZATION ---
print("Generating visualization...")
plt.style.use('seaborn-v0_8-whitegrid')
plt.figure(figsize=(16, 6))

num_bins = int(np.ceil(video_duration_seconds / BIN_WIDTH_SECONDS))

sns.histplot(data=punch_times_seconds, bins=num_bins, kde=False)

plt.title('Punch Distribution Over Time', fontsize=18)
plt.xlabel('Time in Video (seconds)', fontsize=14)
plt.ylabel(f'Number of Punches (per {BIN_WIDTH_SECONDS}s bin)', fontsize=14)
plt.xlim(0, video_duration_seconds)
plt.tight_layout()

print("\nStarting highlight generation using 'Top N Bins' method...")

if punch_detections:
    punch_times_sec = [frame / fps for frame in punch_detections]

    # Count punches in each bin
    num_bins = int(np.ceil(video_duration_seconds / BIN_WIDTH_SECONDS))
    if num_bins == 0: num_bins = 1 # Failsafe for very short videos

    punch_counts, bin_edges = np.histogram(punch_times_sec, bins=num_bins, range=(0, video_duration_seconds))

    # Find the Top N Bins
    top_n_indices = np.argsort(punch_counts)[::-1][:TOP_N_BINS]

    print(f"Identified the Top {TOP_N_BINS} most active {BIN_WIDTH_SECONDS}s intervals.")

    # Clip the Top N Bins as Highlights
    highlight_count = 0
    for bin_index in top_n_indices:
        punch_count_in_bin = punch_counts[bin_index]
        if punch_count_in_bin == 0: continue

        highlight_count += 1

        bin_start_sec = bin_edges[bin_index]
        bin_end_sec = bin_edges[bin_index + 1]

        print(f"\n  -> Found Highlight #{highlight_count}:")
        print(f"     {int(punch_count_in_bin)} punches between {bin_start_sec:.2f}s and {bin_end_sec:.2f}s")

        clip_start_sec = max(0, bin_start_sec - PADDING_SECONDS)
        clip_end_sec = bin_end_sec + PADDING_SECONDS
        start_timestamp = seconds_to_timestamp(clip_start_sec)
        end_timestamp = seconds_to_timestamp(clip_end_sec)

        output_filename = f"highlight_top_{highlight_count}_{start_timestamp.replace(':', '-')}.mp4"
        output_filepath = os.path.join(OUTPUT_CLIPS_FOLDER, output_filename)

        print(f"     Clipping highlight: {output_filename} ({start_timestamp} to {end_timestamp})")

        try:
            ffmpeg.input(INPUT_VIDEO_PATH, ss=start_timestamp, to=end_timestamp).output(output_filepath, c='copy').run(quiet=True, overwrite_output=True)
            print("     -> SUCCESS: Clip saved.")
        except ffmpeg.Error as e:
            print(f"     -> ERROR: {e.stderr.decode()}")

    print(f"\nHighlight generation complete! Saved {highlight_count} highlight clips.")
else:
    print("No punches were loaded, so no highlights could be generated.")

"""# Slow motion Highlights Generation"""

import ffmpeg
import os
import glob
from pydub import AudioSegment
from pydub.utils import make_chunks
import numpy as np
from tqdm import tqdm

# Subfolder where the new slow-motion clips will be saved
OUTPUT_FOLDER_PATH = os.path.join(OUTPUT_CLIPS_FOLDER, 'slow_mo/')

# --- CREATE OUTPUT FOLDER ---
os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)
print(f"Slow-motion clips will be saved to: {OUTPUT_FOLDER_PATH}")

# --- FIND ALL VIDEO CLIPS TO PROCESS ---
video_files = glob.glob(os.path.join(OUTPUT_CLIPS_FOLDER, "*.mp4"))
print(f"Found {len(video_files)} highlight clips to process.")

# --- Parameters ---
SLOWMO_EFFECT_DURATION = 2.0
SLOWDOWN_FACTOR = 3
ANTICIPATION_SECONDS = 2.0
LOUDNESS_INCREASE_DB = 2.5
AUDIO_ANALYSIS_WINDOW_SEC = 1.5

# --- 4. MAIN PROCESSING LOOP ---
for clip_path in tqdm(video_files, desc="Processing Clips"):
    clip_filename = os.path.basename(clip_path)
    output_filepath = os.path.join(OUTPUT_FOLDER_PATH, clip_filename)

    print(f"\n--- Analyzing: {clip_filename} ---")

    # --- Audio Analysis ---
    is_excitement_found = False
    try:
        audio = AudioSegment.from_file(clip_path, "mp4")
        average_loudness = audio.dBFS
        loudness_threshold = average_loudness + LOUDNESS_INCREASE_DB

        chunk_size_ms = 100
        chunks = make_chunks(audio, chunk_size_ms)
        loudness_values = [chunk.dBFS for chunk in chunks]

        # --- UPDATED: Use the new independent window size for analysis ---
        window_size = int(AUDIO_ANALYSIS_WINDOW_SEC * 1000 / chunk_size_ms)
        max_loudness = -1000
        loudest_segment_start_time_ms = 0

        for i in range(len(loudness_values) - window_size):
            window_loudness = np.mean(loudness_values[i:i + window_size])
            if window_loudness > max_loudness:
                max_loudness = window_loudness
                loudest_segment_start_time_ms = i * chunk_size_ms

        if max_loudness > loudness_threshold:
            is_excitement_found = True
            print(f"  -> Major excitement event found peaking at: {loudest_segment_start_time_ms / 1000:.2f} seconds")
        else:
            print(f"  -> No major excitement event found. Skipping slow-motion effect.")

    except Exception as e:
        print(f"  -> Audio analysis failed: {e}")

    # --- Video Editing ---
    if is_excitement_found:
        print(f"  -> Applying slow-motion effect...")
        probe = ffmpeg.probe(clip_path)
        clip_duration = float(probe['format']['duration'])

        # The peak is the start of the loudest analysis window
        start_of_peak = loudest_segment_start_time_ms / 1000

        # The slow-motion effect starts before this peak
        start_of_slowmo_sec = max(0, start_of_peak - ANTICIPATION_SECONDS)

        # The slow-motion effect has its own independent duration
        end_of_slowmo_sec = start_of_slowmo_sec + SLOWMO_EFFECT_DURATION

        if end_of_slowmo_sec > clip_duration:
            end_of_slowmo_sec = clip_duration
            start_of_slowmo_sec = max(0, end_of_slowmo_sec - SLOWMO_EFFECT_DURATION)

        try:
            in_file = ffmpeg.input(clip_path)

            # Create and normalize the three video/audio parts
            vid_part1 = in_file.video.trim(end=start_of_slowmo_sec).filter('setpts', 'PTS-STARTPTS')
            aud_part1 = in_file.audio.filter('atrim', end=start_of_slowmo_sec).filter('asetpts', 'PTS-STARTPTS')

            vid_part2 = in_file.video.trim(start=start_of_slowmo_sec, end=end_of_slowmo_sec).filter('setpts', f'{SLOWDOWN_FACTOR}*(PTS-STARTPTS)')
            aud_part2_trimmed = in_file.audio.filter('atrim', start=start_of_slowmo_sec, end=end_of_slowmo_sec)

            audio_speed = 1 / SLOWDOWN_FACTOR
            aud_part2_slowed = aud_part2_trimmed
            while audio_speed < 0.5:
                aud_part2_slowed = aud_part2_slowed.filter('atempo', 0.5)
                audio_speed /= 0.5
            aud_part2_slowed = aud_part2_slowed.filter('atempo', audio_speed)
            aud_part2 = aud_part2_slowed.filter('asetpts', 'PTS-STARTPTS')

            vid_part3 = in_file.video.trim(start=end_of_slowmo_sec).filter('setpts', 'PTS-STARTPTS')
            aud_part3 = in_file.audio.filter('atrim', start=end_of_slowmo_sec).filter('asetpts', 'PTS-STARTPTS')

            # Concatenate the parts back together
            joined_video = ffmpeg.concat(vid_part1, vid_part2, vid_part3, v=1, a=0)
            joined_audio = ffmpeg.concat(aud_part1, aud_part2, aud_part3, v=0, a=1)

            ffmpeg.output(joined_video, joined_audio, output_filepath).run(quiet=True, overwrite_output=True)

            print(f"  -> ✅ SUCCESS! Saved to {output_filepath}")

        except ffmpeg.Error as e:
            print(f"  -> ERROR: FFmpeg failed for {clip_filename}.")
            # print(e.stderr.decode()) # Uncomment for detailed FFmpeg errors

print("\nBatch processing complete.")

"""# Final Highlights Video Generation"""

import ffmpeg
import os
import glob

# Subfolder where the new slow-motion clips will be saved
FINAL_HIGHLIGHT_REEL_PATH = os.path.join(OUTPUT_FOLDER_PATH, 'finalOut.mp4')

# --- FIND AND SORT THE CLIPS ---
# Find all .mp4 files in the folder
clip_files = sorted(glob.glob(os.path.join(OUTPUT_FOLDER_PATH, "*.mp4")))

if not clip_files:
    print("ERROR: No video clips found in the specified folder.")
else:
    print(f"Found {len(clip_files)} clips to assemble.")

if clip_files:
    try:
        # --- NEW: Get the duration of all clips first ---
        clip_durations = [float(ffmpeg.probe(clip)['format']['duration']) for clip in clip_files]

        # --- 3. CREATE THE FFMPEG FILTER CHAIN ---
        # Load the first clip as the base
        base_video = ffmpeg.input(clip_files[0])
        # Initialize the cumulative duration
        cumulative_duration = clip_durations[0]

        # Iteratively join the remaining clips with a fade transition
        for i in range(1, len(clip_files)):
            next_clip = ffmpeg.input(clip_files[i])

            # The offset is the duration of the combined video so far, minus the fade time
            transition_offset = cumulative_duration - 1.0

            # xfade filter adds a 1-second fade
            base_video = ffmpeg.filter([base_video, next_clip], 'xfade', transition='fade', duration=1, offset=transition_offset)

            # Update the cumulative duration for the next iteration
            cumulative_duration += clip_durations[i] - 1.0

        # --- 4. EXECUTE AND SAVE THE FINAL REEL ---
        print("\nAssembling the final highlight reel... This may take a few minutes.")

        (
            base_video
            .output(FINAL_HIGHLIGHT_REEL_PATH)
            .run(quiet=True, overwrite_output=True)
        )

        print(f"Success! Final highlight reel saved to: {FINAL_HIGHLIGHT_REEL_PATH}")

    except ffmpeg.Error as e:
        print(f"ERROR: FFmpeg failed.")
        print(e.stderr.decode())