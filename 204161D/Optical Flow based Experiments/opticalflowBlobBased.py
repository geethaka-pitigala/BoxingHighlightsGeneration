# -*- coding: utf-8 -*-
"""OpticalFlowBlobBased.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15L0UnXeI8WxxRqktUi8wMZMBkRI9FnPu

# Setup
"""

# Cell 1: Setup and Installations
from google.colab import drive
import os

# Mount your Google Drive
drive.mount('/content/drive')

# Install the necessary libraries
!pip install ultralytics -q

print("✅ Setup Complete")

# Cell 2: Imports and Configuration
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from google.colab import files

# --- File and Model Configuration ---
# ⚠️ Update these paths
VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/input/round_small1.mp4'
YOLO_MODEL_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/playerIdentificationModel2.pt'
OUTPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/output/round_small1_13.mp4'

# Cell 3: Model Loading
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = YOLO(YOLO_MODEL_PATH)
model.to(device)

print(f"✅ Model loaded from {YOLO_MODEL_PATH}")

"""# Parameters"""

# --- Detection and Thresholding Parameters ---
PLAYER_CONFIDENCE_THRESHOLD = 0.5
# ✅ Static threshold in pixels per frame.
MOTION_MAGNITUDE_THRESHOLD = 10.0

# ✅ New parameter to control noise removal strength.
# Increase this value (e.g., 7, 9) for more aggressive noise removal.
NOISE_REMOVAL_KERNEL_SIZE = 5

"""# Code"""

# Cell 4: Core Processing Functions

def detect_players(frame: np.ndarray, model: YOLO, confidence_threshold: float) -> tuple | None:
    """Detects two players in a frame using the YOLO model."""
    results = model(frame, verbose=False)
    boxes = results[0].boxes.cpu().numpy()
    confident_boxes = [box for box in boxes if box.conf[0] > confidence_threshold]

    if len(confident_boxes) >= 2:
        confident_boxes.sort(key=lambda x: x.conf[0], reverse=True)
        box1 = confident_boxes[0].xyxy[0].astype(int)
        box2 = confident_boxes[1].xyxy[0].astype(int)
        return (box1, box2)
    return None

def calculate_roi(box1: np.ndarray, box2: np.ndarray, frame_shape: tuple, margin: int = 30) -> tuple:
    """Calculates a single Region of Interest (ROI) that contains both player boxes."""
    frame_h, frame_w = frame_shape
    x_min, y_min = min(box1[0], box2[0]), min(box1[1], box2[1])
    x_max, y_max = max(box1[2], box2[2]), max(box1[3], box2[3])
    x = max(0, x_min - margin)
    y = max(0, y_min - margin)
    w = min(frame_w, x_max + margin) - x
    h = min(frame_h, y_max + margin) - y
    return (x, y, w, h)

def calculate_optical_flow(prev_gray: np.ndarray, current_gray: np.ndarray) -> np.ndarray | None:
    """Calculates dense optical flow between two grayscale frames."""
    if prev_gray.shape != current_gray.shape: return None
    return cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

def get_cleaned_motion_mask(flow: np.ndarray, magnitude_threshold: float) -> np.ndarray:
    """Thresholds the optical flow and cleans the resulting mask."""
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    mask = (magnitude > magnitude_threshold).astype(np.uint8)

    # Clean the mask using morphological opening to remove small noise
    kernel = np.ones((5, 5), np.uint8)
    mask_opened = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    return mask_opened

# Cell 5: Main Processing Loop

cap = cv2.VideoCapture(VIDEO_PATH)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (frame_width, frame_height))

ret, prev_frame = cap.read()
if ret:
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
else:
    print("Error reading the first frame.")
    cap.release()

frame_count = 0
while ret:
    output_frame = np.zeros_like(prev_frame)
    current_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    player_boxes = detect_players(prev_frame, model, PLAYER_CONFIDENCE_THRESHOLD)
    status_text = "Status: Detecting Players..."

    if player_boxes:
        status_text = "Status: Motion Detected"
        box1, box2 = player_boxes
        rx, ry, rw, rh = calculate_roi(box1, box2, (frame_height, frame_width))

        flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

        if flow is not None:
            # Use the static pixel-based threshold from the configuration
            motion_mask = get_cleaned_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD)

            roi_original_color = prev_frame[ry:ry+rh, rx:rx+rw]
            debug_view = cv2.bitwise_and(roi_original_color, roi_original_color, mask=motion_mask)

            output_frame[ry:ry+rh, rx:rx+rw] = debug_view

    cv2.putText(output_frame, f"Frame: {frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

    writer.write(output_frame)

    prev_gray = current_gray
    ret, prev_frame = cap.read()
    frame_count += 1

print(f"\n✅ Processing complete. Processed {frame_count-1} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()

"""# with Noise Removal"""

def get_cleaned_motion_mask(flow: np.ndarray, magnitude_threshold: float, kernel_size: int) -> np.ndarray:
    """Thresholds the optical flow and cleans the resulting mask with a configurable kernel."""
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    mask = (magnitude > magnitude_threshold).astype(np.uint8)

    # Use the configurable kernel size for morphological opening
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    mask_opened = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    return mask_opened

# Cell 5: Main Processing Loop

cap = cv2.VideoCapture(VIDEO_PATH)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (frame_width, frame_height))

ret, prev_frame = cap.read()
if ret:
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
else:
    print("Error reading the first frame.")
    cap.release()

frame_count = 0
while ret:
    output_frame = np.zeros_like(prev_frame)
    current_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    player_boxes = detect_players(prev_frame, model, PLAYER_CONFIDENCE_THRESHOLD)
    status_text = "Status: Detecting Players..."

    if player_boxes:
        status_text = "Status: Motion Detected"
        box1, box2 = player_boxes
        rx, ry, rw, rh = calculate_roi(box1, box2, (frame_height, frame_width))

        flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

        if flow is not None:
            # Pass the new kernel size parameter to the function
            motion_mask = get_cleaned_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD, NOISE_REMOVAL_KERNEL_SIZE)

            roi_original_color = prev_frame[ry:ry+rh, rx:rx+rw]
            debug_view = cv2.bitwise_and(roi_original_color, roi_original_color, mask=motion_mask)

            output_frame[ry:ry+rh, rx:rx+rw] = debug_view

    cv2.putText(output_frame, f"Frame: {frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

    writer.write(output_frame)

    prev_gray = current_gray
    ret, prev_frame = cap.read()
    frame_count += 1

print(f"\n✅ Processing complete. Processed {frame_count-1} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()

"""# Standardized Resolution and FPS"""

# --- Standardization and Processing Parameters ---
# ✅ The fixed width to resize all frames to.
TARGET_WIDTH = 640
# ✅ The fixed FPS to process the video at. Frames will be skipped to meet this.
TARGET_FPS = 25

# --- Detection and Thresholding Parameters ---
PLAYER_CONFIDENCE_THRESHOLD = 0.5
# ✅ Static threshold in pixels per frame, which is now reliable due to fixed FPS and resolution.
MOTION_MAGNITUDE_THRESHOLD = 5.0
NOISE_REMOVAL_KERNEL_SIZE = 5

# Cell 4: Core Processing Functions

def detect_players(frame: np.ndarray, model: YOLO, confidence_threshold: float) -> tuple | None:
    results = model(frame, verbose=False)
    boxes = results[0].boxes.cpu().numpy()
    confident_boxes = [box for box in boxes if box.conf[0] > confidence_threshold]

    if len(confident_boxes) >= 2:
        confident_boxes.sort(key=lambda x: x.conf[0], reverse=True)
        box1 = confident_boxes[0].xyxy[0].astype(int)
        box2 = confident_boxes[1].xyxy[0].astype(int)
        return (box1, box2)
    return None

def calculate_roi(box1: np.ndarray, box2: np.ndarray, frame_shape: tuple, margin: int = 30) -> tuple:
    frame_h, frame_w = frame_shape
    x_min, y_min = min(box1[0], box2[0]), min(box1[1], box2[1])
    x_max, y_max = max(box1[2], box2[2]), max(box1[3], box2[3])
    x = max(0, x_min - margin)
    y = max(0, y_min - margin)
    w = min(frame_w, x_max + margin) - x
    h = min(frame_h, y_max + margin) - y
    return (x, y, w, h)

def calculate_optical_flow(prev_gray: np.ndarray, current_gray: np.ndarray) -> np.ndarray | None:
    if prev_gray.shape != current_gray.shape: return None
    return cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

def get_cleaned_motion_mask(flow: np.ndarray, magnitude_threshold: float, kernel_size: int) -> np.ndarray:
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    mask = (magnitude > magnitude_threshold).astype(np.uint8)
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    mask_opened = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    return mask_opened

# Cell 5: Main Processing Loop

cap = cv2.VideoCapture(VIDEO_PATH)
# Get original video properties
source_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
source_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
source_fps = cap.get(cv2.CAP_PROP_FPS)

# --- Standardization Setup ---
# Calculate the new height to maintain aspect ratio
aspect_ratio = source_height / source_width
target_height = int(TARGET_WIDTH * aspect_ratio)
target_dim = (TARGET_WIDTH, target_height)

# Calculate how many frames to skip to achieve the TARGET_FPS
if source_fps > 0:
    frame_skip = round(source_fps / TARGET_FPS)
else:
    print("Warning: Source FPS is 0. Defaulting to no frame skipping.")
    frame_skip = 1

# --- Video Writer Setup ---
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, TARGET_FPS, target_dim)

# --- Loop Initialization ---
ret, frame = cap.read()
if ret:
    prev_frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
    prev_gray = cv2.cvtColor(prev_frame_resized, cv2.COLOR_BGR2GRAY)
else:
    print("Error reading the first frame.")
    cap.release()

frame_idx = 0
while ret:
    # --- Frame Skipping Logic ---
    # Only process the frame if it's one we want based on the skip rate
    if frame_idx % frame_skip == 0:
        # Resize the current frame
        frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
        output_frame = np.zeros_like(frame_resized)
        current_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)

        player_boxes = detect_players(frame_resized, model, PLAYER_CONFIDENCE_THRESHOLD)
        status_text = "Status: Detecting Players..."

        if player_boxes:
            status_text = "Status: Motion Detected"
            box1, box2 = player_boxes
            rx, ry, rw, rh = calculate_roi(box1, box2, (target_height, TARGET_WIDTH))

            flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

            if flow is not None:
                motion_mask = get_cleaned_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD, NOISE_REMOVAL_KERNEL_SIZE)

                roi_original_color = frame_resized[ry:ry+rh, rx:rx+rw]
                debug_view = cv2.bitwise_and(roi_original_color, roi_original_color, mask=motion_mask)

                output_frame[ry:ry+rh, rx:rx+rw] = debug_view

        cv2.putText(output_frame, f"Frame: {frame_idx}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        writer.write(output_frame)

        # Update the 'previous' frame state only when we've processed a frame
        prev_gray = current_gray

    # Read the next frame from the source video
    ret, frame = cap.read()
    frame_idx += 1

print(f"\n✅ Processing complete. Processed {frame_idx // frame_skip} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()

"""# Blob Detection"""

# ✅ Dynamic blob size thresholds defined as a RATIO of the player's bounding box
MIN_BLOB_AREA_RATIO = 0.005  # 0.5% of the bbox area
MAX_BLOB_AREA_RATIO = 0.05   # 5% of the bbox area

def find_motion_blobs(mask: np.ndarray, min_area: int, max_area: int) -> list:
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    blobs = []
    for contour in contours:
        area = cv2.contourArea(contour)
        if min_area < area < max_area:
            M = cv2.moments(contour)
            if M['m00'] == 0: continue
            cx = int(M['m10'] / M['m00'])
            cy = int(M['m01'] / M['m00'])
            blobs.append({'contour': contour, 'area': area, 'centroid': (cx, cy)})
    return blobs

# Cell 5: Main Processing Loop

cap = cv2.VideoCapture(VIDEO_PATH)
source_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
source_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
source_fps = cap.get(cv2.CAP_PROP_FPS)

aspect_ratio = source_height / source_width
target_height = int(TARGET_WIDTH * aspect_ratio)
target_dim = (TARGET_WIDTH, target_height)

if source_fps > 0:
    frame_skip = round(source_fps / TARGET_FPS)
else:
    frame_skip = 1

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, TARGET_FPS, target_dim)

ret, frame = cap.read()
if ret:
    prev_frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
    prev_gray = cv2.cvtColor(prev_frame_resized, cv2.COLOR_BGR2GRAY)
else:
    cap.release()

frame_idx = 0
while ret:
    if frame_idx % frame_skip == 0:
        frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
        output_frame = np.zeros_like(frame_resized)
        current_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)

        player_boxes = detect_players(frame_resized, model, PLAYER_CONFIDENCE_THRESHOLD)
        status_text = "Status: Detecting Players..."

        if player_boxes:
            box1, box2 = player_boxes

            # --- Dynamic Blob Size Calculation ---
            bbox_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
            dynamic_min_area = int(bbox_area * MIN_BLOB_AREA_RATIO)
            dynamic_max_area = int(bbox_area * MAX_BLOB_AREA_RATIO)

            rx, ry, rw, rh = calculate_roi(box1, box2, (target_height, TARGET_WIDTH))

            flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

            if flow is not None:
                motion_mask = get_cleaned_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD, NOISE_REMOVAL_KERNEL_SIZE)

                # Use the dynamic min/max area for filtering
                blobs = find_motion_blobs(motion_mask, dynamic_min_area, dynamic_max_area)

                status_text = f"Status: Found {len(blobs)} motion blobs"

                roi_original_color = frame_resized[ry:ry+rh, rx:rx+rw]
                debug_view = cv2.bitwise_and(roi_original_color, roi_original_color, mask=motion_mask)

                for blob in blobs:
                    cv2.drawContours(debug_view, [blob['contour']], -1, (0, 255, 0), 2)

                output_frame[ry:ry+rh, rx:rx+rw] = debug_view

        cv2.putText(output_frame, f"Frame: {frame_idx}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        writer.write(output_frame)

        prev_gray = current_gray

    ret, frame = cap.read()
    frame_idx += 1

print(f"\n✅ Processing complete. Processed {frame_idx // frame_skip} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()

"""# Punch Tracking and Detection

In this approach we are trying to isolate the moving glove blob using the thresholding and use those isolated blobs in the black background as the candiates for check for punches across frames.
"""

# ✅ NEW: Stricter Tracking and Punch Classification Parameters
TRACKING_MAX_DISTANCE = 50
TRACK_INACTIVITY_FRAMES = 3
# --- Punch Rule Parameters ---
PUNCH_SPEED_THRESHOLD = 20        # Avg speed over recent frames
MIN_TRACK_LENGTH_FOR_PUNCH = 4    # Min history points to be a punch
SUSTAINED_SPEED_FRAME_COUNT = 3   # How many recent frames to check for speed
CONSISTENT_DIRECTION_THRESHOLD = 0.7 # Min dot product for direction consistency

# Cell 5: New Punch Tracker Class

class PunchTracker:
    def __init__(self):
        self.next_track_id = 0
        self.tracks = []

    def update(self, blobs):
        # Mark all tracks as potentially inactive
        for track in self.tracks:
            track['inactive_for'] += 1

        # Match new blobs with existing tracks
        unmatched_blobs = list(range(len(blobs)))
        for track in self.tracks:
            last_pos = track['history'][-1]
            best_match_dist = float('inf')
            best_match_idx = -1

            for i in unmatched_blobs:
                dist = np.linalg.norm(np.array(last_pos) - np.array(blobs[i]['centroid']))
                if dist < TRACKING_MAX_DISTANCE and dist < best_match_dist:
                    best_match_dist = dist
                    best_match_idx = i

            if best_match_idx != -1:
                # Update the matched track
                track['contour'] = blobs[best_match_idx]['contour']
                track['history'].append(blobs[best_match_idx]['centroid'])
                track['inactive_for'] = 0
                unmatched_blobs.remove(best_match_idx)

        # Create new tracks for unmatched blobs
        for idx in unmatched_blobs:
            self.tracks.append({
                'id': self.next_track_id,
                'contour': blobs[idx]['contour'],
                'history': [blobs[idx]['centroid']],
                'inactive_for': 0,
                'is_punch': False # Always start as False
            })
            self.next_track_id += 1

        # Remove stale tracks
        self.tracks = [t for t in self.tracks if t['inactive_for'] < TRACK_INACTIVITY_FRAMES]

        # ✅ NEW: Classify each active track in every frame
        for track in self.tracks:
            self._classify_track(track)

    def _classify_track(self, track):
        """Applies a set of rules to classify a track as a punch."""
        # Assume it's not a punch until it passes all checks
        track['is_punch'] = False

        # --- Rule 1: Minimum History ---
        if len(track['history']) < MIN_TRACK_LENGTH_FOR_PUNCH:
            return

        # --- Rule 2: Sustained High Speed ---
        recent_history = track['history'][-SUSTAINED_SPEED_FRAME_COUNT:]
        speeds = [np.linalg.norm(np.array(recent_history[i]) - np.array(recent_history[i-1])) for i in range(1, len(recent_history))]
        if not speeds or (sum(speeds) / len(speeds)) < PUNCH_SPEED_THRESHOLD:
            return

        # --- Rule 3: Consistent Direction ---
        vectors = [np.array(recent_history[i]) - np.array(recent_history[i-1]) for i in range(1, len(recent_history))]
        normalized_vectors = []
        for vec in vectors:
            norm = np.linalg.norm(vec)
            if norm > 0:
                normalized_vectors.append(vec / norm)

        if len(normalized_vectors) < 2:
            return

        dot_products = [np.dot(normalized_vectors[i], normalized_vectors[i-1]) for i in range(1, len(normalized_vectors))]
        if (sum(dot_products) / len(dot_products)) < CONSISTENT_DIRECTION_THRESHOLD:
            return

        # ✅ If all checks pass, classify as a punch for this frame
        track['is_punch'] = True

# Cell 6: Main Processing Loop

# --- Initialization ---
cap = cv2.VideoCapture(VIDEO_PATH)
source_width, source_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
source_fps = cap.get(cv2.CAP_PROP_FPS)

aspect_ratio = source_height / source_width
target_height = int(TARGET_WIDTH * aspect_ratio)
target_dim = (TARGET_WIDTH, target_height)

frame_skip = round(source_fps / TARGET_FPS) if source_fps > 0 else 1

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, TARGET_FPS, target_dim)

tracker = PunchTracker() # ✅ Initialize the tracker

ret, frame = cap.read()
if ret:
    prev_frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
    prev_gray = cv2.cvtColor(prev_frame_resized, cv2.COLOR_BGR2GRAY)
else:
    cap.release()

frame_idx = 0
while ret:
    if frame_idx % frame_skip == 0:
        frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
        current_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)

        # We draw on the original color frame for the final output
        output_frame = frame_resized.copy()

        player_boxes = detect_players(frame_resized, model, PLAYER_CONFIDENCE_THRESHOLD)
        status_text = "Status: Detecting Players..."

        if player_boxes:
            box1, box2 = player_boxes
            bbox_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
            dynamic_min_area = int(bbox_area * MIN_BLOB_AREA_RATIO)
            dynamic_max_area = int(bbox_area * MAX_BLOB_AREA_RATIO)

            rx, ry, rw, rh = calculate_roi(box1, box2, (target_height, TARGET_WIDTH))

            flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

            if flow is not None:
                motion_mask = get_cleaned_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD, NOISE_REMOVAL_KERNEL_SIZE)
                blobs = find_motion_blobs(motion_mask, dynamic_min_area, dynamic_max_area)

                # ✅ Update tracker with blobs relative to ROI
                # We need to offset blob centroids by the ROI origin (rx, ry)
                for blob in blobs:
                    blob['centroid'] = (blob['centroid'][0] + rx, blob['centroid'][1] + ry)

                tracker.update(blobs)

                status_text = f"Status: Tracking {len(tracker.tracks)} objects"

                # ✅ Draw final annotations based on tracker results
                for track in tracker.tracks:
                    x, y, w, h = cv2.boundingRect(track['contour'])
                    # Offset the blob's bounding box by ROI origin
                    x, y = x + rx, y + ry

                    if track['is_punch']:
                        color = (0, 0, 255) # Red for a punch
                        cv2.rectangle(output_frame, (x,y), (x+w, y+h), color, 3)
                        cv2.putText(output_frame, "PUNCH", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
                    else:
                        color = (0, 255, 0) # Green for normal motion
                        cv2.rectangle(output_frame, (x,y), (x+w, y+h), color, 2)

        cv2.putText(output_frame, f"Frame: {frame_idx}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        writer.write(output_frame)
        prev_gray = current_gray

    ret, frame = cap.read()
    frame_idx += 1

print(f"\n✅ Processing complete. Processed {frame_idx // frame_skip} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()