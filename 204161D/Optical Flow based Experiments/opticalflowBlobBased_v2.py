# -*- coding: utf-8 -*-
"""OpticalFlowBasedBlobBasedv2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/164OH6bvOCY-2SQP_hGGgZXW0OaDUFMWb

# Setup
"""

# Cell 1: Setup and Installations
from google.colab import drive
import os

# Mount your Google Drive
drive.mount('/content/drive')

# Install the necessary libraries
!pip install ultralytics -q

print("✅ Setup Complete")

# Cell 2: Imports and Configuration
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from google.colab import files
from sklearn.cluster import DBSCAN
from random import randint

# --- File and Model Configuration ---
# ⚠️ Update these paths
VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/input/input1.mp4'
YOLO_MODEL_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/playerIdentificationModel2.pt'
OUTPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/output/input1_32.mp4'

# Cell 3: Model Loading
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = YOLO(YOLO_MODEL_PATH)
model.to(device)

print(f"✅ Model loaded from {YOLO_MODEL_PATH}")

"""## Parameters"""

# --- Standardization and Processing Parameters ---
TARGET_WIDTH = 640
TARGET_FPS = 25

# --- Detection and Pre-filtering Parameters ---
PLAYER_CONFIDENCE_THRESHOLD = 0.5
# A forgiving threshold to get a general mask of moving pixels
MOTION_MAGNITUDE_THRESHOLD = 1.0

# --- DBSCAN Clustering Parameters ---
# The max distance between two samples for one to be considered as in the neighborhood of the other.
# This is the most important DBSCAN parameter to tune.
DBSCAN_EPS = 10
DBSCAN_MIN_SAMPLES = 15
# Weights to balance the importance of position vs. motion during clustering
POSITION_WEIGHT = 1.0
FLOW_WEIGHT = 4.0 # Give motion more importance

# --- Cluster Filtering Parameters ---
MIN_CLUSTER_SIZE = 10  # Min pixels for a valid motion cluster
MAX_CLUSTER_SIZE = 500 # Max pixels for a valid motion cluster

MIN_PUNCH_SPEED = 10.0

# ✅ NEW: Tracking and Punch Classification Rules
TRACKING_MAX_DISTANCE = 75          # Max pixels a cluster can move to be considered the same track.
TRACK_INACTIVITY_FRAMES = 3         # How many frames a track can be "lost" before being deleted.
MIN_TRACK_LENGTH_FOR_PUNCH = 4      # The track must exist for at least this many frames.
PUNCH_SPEED_THRESHOLD = 20          # Avg. speed (pixels/frame) over recent frames to be a punch.
SUSTAINED_SPEED_FRAME_COUNT = 3     # The number of recent frames to check for sustained speed.
CONSISTENT_DIRECTION_THRESHOLD = 0.7 # Minimum dot product for direction consistency.

"""## Processing"""

def detect_players(frame: np.ndarray, model: YOLO, confidence_threshold: float) -> tuple | None:
    """Detects two players in a frame using the YOLO model."""
    results = model(frame, verbose=False)
    boxes = results[0].boxes.cpu().numpy()
    confident_boxes = [box for box in boxes if box.conf[0] > confidence_threshold]

    if len(confident_boxes) >= 2:
        confident_boxes.sort(key=lambda x: x.conf[0], reverse=True)
        box1 = confident_boxes[0].xyxy[0].astype(int)
        box2 = confident_boxes[1].xyxy[0].astype(int)
        return (box1, box2)
    return None

def calculate_roi(box1: np.ndarray, box2: np.ndarray, frame_shape: tuple, margin: int = 30) -> tuple:
    """Calculates a single Region of Interest (ROI) that contains both player boxes."""
    frame_h, frame_w = frame_shape
    x_min, y_min = min(box1[0], box2[0]), min(box1[1], box2[1])
    x_max, y_max = max(box1[2], box2[2]), max(box1[3], box2[3])
    x = max(0, x_min - margin)
    y = max(0, y_min - margin)
    w = min(frame_w, x_max + margin) - x
    h = min(frame_h, y_max + margin) - y
    return (x, y, w, h)

def calculate_optical_flow(prev_gray: np.ndarray, current_gray: np.ndarray) -> np.ndarray | None:
    """Calculates dense optical flow between two grayscale frames."""
    if prev_gray.shape != current_gray.shape: return None
    return cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

def get_motion_mask(flow: np.ndarray, magnitude_threshold: float) -> np.ndarray:
    """Creates a simple binary mask of pixels with motion above a threshold."""
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    mask = (magnitude > magnitude_threshold).astype(np.uint8)
    return mask

def cluster_motion_with_dbscan(flow: np.ndarray, mask: np.ndarray) -> list:
    """
    Groups moving pixels into clusters based on proximity and motion similarity using DBSCAN.
    """
    # Get coordinates of all moving pixels from the mask
    pixels = np.argwhere(mask > 0)
    if len(pixels) < DBSCAN_MIN_SAMPLES:
        return [] # Not enough points to cluster

    # Invert (y, x) from argwhere to (x, y) for spatial coordinates
    pixels_xy = pixels[:, ::-1]

    # Get the flow vectors (u, v) for each of these pixels
    flow_vectors = flow[pixels[:, 0], pixels[:, 1]]

    # Create the feature set for clustering: [pos_x, pos_y, flow_u, flow_v]
    # We apply weights to balance their importance
    features = np.hstack([
        pixels_xy * POSITION_WEIGHT,
        flow_vectors * FLOW_WEIGHT
    ])

    # Perform DBSCAN clustering
    clustering = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(features)
    labels = clustering.labels_

    # Organize pixels by cluster label
    # Ignore noise cluster (label -1)
    unique_labels = np.unique(labels)
    clusters = []
    for label in unique_labels:
        if label == -1:
            continue

        cluster_pixels = pixels[labels == label]

        # Filter clusters by size
        if MIN_CLUSTER_SIZE < len(cluster_pixels) < MAX_CLUSTER_SIZE:
            clusters.append(cluster_pixels)

    return clusters

def find_candidate_clusters(flow: np.ndarray, mask: np.ndarray) -> list:
    """
    Clusters moving pixels and filters them by size to find all potential punch candidates.
    Returns: A list of candidate cluster dictionaries.
    """
    pixels = np.argwhere(mask > 0)
    if len(pixels) < DBSCAN_MIN_SAMPLES: return []

    features = np.hstack([pixels[:, ::-1] * POSITION_WEIGHT, flow[pixels[:, 0], pixels[:, 1]] * FLOW_WEIGHT])
    labels = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(features).labels_

    candidates = []
    for label in np.unique(labels):
        if label == -1: continue

        cluster_pixels = pixels[labels == label]
        if MIN_CLUSTER_SIZE < len(cluster_pixels) < MAX_CLUSTER_SIZE:
            # Calculate centroid for tracking
            M = cv2.moments(cluster_pixels[:, ::-1]) # moments requires (x,y)
            if M['m00'] == 0: continue
            cx = int(M['m10'] / M['m00'])
            cy = int(M['m01'] / M['m00'])

            candidates.append({'pixels': cluster_pixels, 'centroid': (cx, cy)})

    return candidates

# Cell 5: New Punch Tracker Class

class PunchTracker:
    def __init__(self):
        self.next_track_id = 0
        self.tracks = []

    def _classify_track(self, track):
        """Applies a set of rules to classify a track as a punch."""
        track['is_punch'] = False # Reset classification each frame

        # Rule 1: Minimum History
        if len(track['history']) < MIN_TRACK_LENGTH_FOR_PUNCH:
            return

        # Rule 2: Sustained High Speed
        recent_history = track['history'][-SUSTAINED_SPEED_FRAME_COUNT:]
        speeds = [np.linalg.norm(np.array(recent_history[i]) - np.array(recent_history[i-1])) for i in range(1, len(recent_history))]
        if not speeds or (sum(speeds) / len(speeds)) < PUNCH_SPEED_THRESHOLD:
            return

        # Rule 3: Consistent Direction
        vectors = [np.array(recent_history[i]) - np.array(recent_history[i-1]) for i in range(1, len(recent_history))]
        normalized_vectors = []
        for vec in vectors:
            norm = np.linalg.norm(vec)
            if norm > 0: normalized_vectors.append(vec / norm)

        if len(normalized_vectors) < 2: return

        dot_products = [np.dot(normalized_vectors[i], normalized_vectors[i-1]) for i in range(1, len(normalized_vectors))]
        if (sum(dot_products) / len(dot_products)) < CONSISTENT_DIRECTION_THRESHOLD:
            return

        # If all checks pass, classify as a punch
        track['is_punch'] = True

    def update(self, candidates):
        # Mark tracks as potentially inactive
        for track in self.tracks:
            track['inactive_for'] += 1

        # Match candidates to existing tracks
        unmatched_candidates = list(range(len(candidates)))
        for track in self.tracks:
            last_pos = track['history'][-1]
            best_match_dist = float('inf')
            best_match_idx = -1

            for i in unmatched_candidates:
                dist = np.linalg.norm(np.array(last_pos) - np.array(candidates[i]['centroid']))
                if dist < TRACKING_MAX_DISTANCE and dist < best_match_dist:
                    best_match_dist = dist
                    best_match_idx = i

            if best_match_idx != -1:
                track['pixels'] = candidates[best_match_idx]['pixels']
                track['history'].append(candidates[best_match_idx]['centroid'])
                track['inactive_for'] = 0
                unmatched_candidates.remove(best_match_idx)

        # Create new tracks
        for idx in unmatched_candidates:
            self.tracks.append({
                'id': self.next_track_id,
                'pixels': candidates[idx]['pixels'],
                'history': [candidates[idx]['centroid']],
                'inactive_for': 0,
                'is_punch': False
            })
            self.next_track_id += 1

        # Remove stale tracks and classify active ones
        self.tracks = [t for t in self.tracks if t['inactive_for'] < TRACK_INACTIVITY_FRAMES]
        for track in self.tracks:
            self._classify_track(track)

# Cell 6: Main Processing Loop

# --- Initialization ---
cap = cv2.VideoCapture(VIDEO_PATH)
source_width, source_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
source_fps = cap.get(cv2.CAP_PROP_FPS)

aspect_ratio = source_height / source_width
target_height = int(TARGET_WIDTH * aspect_ratio)
target_dim = (TARGET_WIDTH, target_height)
frame_skip = round(source_fps / TARGET_FPS) if source_fps > 0 else 1

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, TARGET_FPS, target_dim)

tracker = PunchTracker() # Initialize the tracker

ret, frame = cap.read()
if ret:
    prev_frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
    prev_gray = cv2.cvtColor(prev_frame_resized, cv2.COLOR_BGR2GRAY)
else:
    cap.release()

# --- Main Loop ---
frame_idx = 0
while ret:
    if frame_idx % frame_skip == 0:
        frame_resized = cv2.resize(frame, target_dim, interpolation=cv2.INTER_AREA)
        output_frame = frame_resized.copy() # Annotate on the color frame
        current_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)

        player_boxes = detect_players(frame_resized, model, PLAYER_CONFIDENCE_THRESHOLD)
        status_text = "Status: Detecting Players..."

        if player_boxes:
            rx, ry, rw, rh = calculate_roi(player_boxes[0], player_boxes[1], (target_height, TARGET_WIDTH))
            flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

            if flow is not None:
                motion_mask = get_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD)
                candidates = find_candidate_clusters(flow, motion_mask)

                # Offset candidate centroids to be in full-frame coordinates
                for cand in candidates:
                    cand['centroid'] = (cand['centroid'][0] + rx, cand['centroid'][1] + ry)

                tracker.update(candidates)

                # Visualize the results from the tracker
                # punch_count = 0
                # for track in tracker.tracks:
                #     if track['is_punch']:
                #         punch_count += 1
                #         # Highlight punch clusters in red
                #         for pixel in track['pixels']:
                #             abs_y, abs_x = ry + pixel[0], rx + pixel[1]
                #             cv2.circle(output_frame, (abs_x, abs_y), 1, (0, 0, 255), -1)

                # status_text = f"Status: Tracking {len(tracker.tracks)} objects. Punches: {punch_count}"
                # --- TEMPORARY DEBUG VISUALIZATION ---
                # Offset candidate centroids and pixels to be in full-frame coordinates
                punch_count = len(candidates) # for status text
                for cand in candidates:
                    color = (randint(60, 255), randint(60, 255), randint(60, 255))
                    for pixel in cand['pixels']:
                        abs_y, abs_x = ry + pixel[0], rx + pixel[1]
                        cv2.circle(output_frame, (abs_x, abs_y), 1, color, -1)

                status_text = f"Status: Found {punch_count} candidate clusters"

        cv2.putText(output_frame, f"Frame: {frame_idx}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

        writer.write(output_frame)
        prev_gray = current_gray

    ret, frame = cap.read()
    frame_idx += 1

print(f"\n✅ Processing complete. Processed {frame_idx // frame_skip} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()