# -*- coding: utf-8 -*-
"""OpticalFlowClusteringStep2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ojksiNzgp7Kc5sFtq7jwubfmXxhp2X5Y
"""

from google.colab import drive
drive.mount('/content/drive')

# Standard Libraries
import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# Import for K-Means Clustering
from sklearn.cluster import MiniBatchKMeans

print("✅ Step 1/5: Environment is set up with scikit-learn.")

# --- Path Configuration ---
INPUT_VIDEO_PATH = "/content/drive/MyDrive/University/Research/optical flow and glove detection/input/input1.mp4"
# New output name for this step
OUTPUT_VIDEO_PATH = "/content/drive/MyDrive/University/Research/optical flow and glove detection/output/clustering_step3_3.mp4"

# --- Processing Configuration ---
REDUCED_WIDTH = 480

# --- HYBRID DETECTION CONFIGURATION ---
# The lenient threshold to remove static pixels BEFORE clustering.
# This is the key parameter for our pre-filtering step.
PRE_FILTER_THRESHOLD = 0.75

# The number of motion clusters to find within the *active* pixels.
# Since the static pixels are removed, we can often use fewer clusters.
N_CLUSTERS = 3

# We can still sample the active pixels if needed, but the set is already smaller.
SAMPLING_FACTOR = 5


# --- Verification and Setup ---
if not os.path.exists(INPUT_VIDEO_PATH):
    print(f"❌ ERROR: Input video not found at '{INPUT_VIDEO_PATH}'")
else:
    output_dir = os.path.dirname(OUTPUT_VIDEO_PATH)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print(f"▶️ Input video: {INPUT_VIDEO_PATH}")
    print(f"◀️ Output video will be saved to: {OUTPUT_VIDEO_PATH}")
    print("✅ Step 2/5: Configuration is set for the HYBRID (Threshold + Cluster) approach.")

def get_hybrid_cluster_visualization(flow, pre_filter_threshold, n_clusters, sampling_factor):
    """
    First thresholds flow to find active pixels, then clusters only those
    active pixels and returns a colored visualization.
    Args:
        flow: The dense optical flow output (h, w, 2).
        pre_filter_threshold: The initial magnitude threshold to find moving pixels.
        n_clusters: The number of clusters to find within the active pixels.
        sampling_factor: The factor by which to downsample active pixels.
    Returns:
        A color image (h, w, 3) where each color represents a motion cluster.
    """
    h, w, _ = flow.shape

    # --- STAGE 1: Threshold Pre-filtering ---
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    # Create a boolean mask of pixels that have at least some motion
    active_pixels_mask = magnitude > pre_filter_threshold

    # --- STAGE 2: Cluster ONLY the Active Pixels ---
    # Get the flow vectors ONLY for the pixels that passed the threshold
    active_flow_vectors = flow[active_pixels_mask]

    # Create a blank image for the visualization first
    viz_image = np.zeros((h, w, 3), dtype=np.uint8)

    # If there are not enough moving pixels to form clusters, just return
    if active_flow_vectors.shape[0] < n_clusters:
        return viz_image

    # Use MiniBatchKMeans on the smaller, pre-filtered set of vectors
    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0, n_init='auto')
    kmeans.fit(active_flow_vectors)

    # Predict the cluster label for every active pixel
    predicted_labels = kmeans.predict(active_flow_vectors)

    # --- Visualization ---
    colors = [
        (0, 255, 0),    # Green for the slowest moving cluster
        (0, 255, 255),  # Yellow for medium
        (0, 0, 255),    # Red for the fastest
        (255, 0, 0),    # Blue
    ]

    # To make the colors meaningful, let's sort them by speed
    cluster_centers_speed = np.linalg.norm(kmeans.cluster_centers_, axis=1)
    # Get the indices that would sort the speeds in ascending order
    sorted_speed_indices = np.argsort(cluster_centers_speed)

    # Create a mapping from the original kmeans label to a speed-ordered color
    # e.g., the slowest cluster will always be green, fastest will be red
    color_map = {original_label: colors[rank] for rank, original_label in enumerate(sorted_speed_indices)}

    # Assign the correct color to each active pixel in the visualization image
    # Note: We create a temporary array to hold the color for each active pixel
    colored_active_pixels = [color_map[label] for label in predicted_labels]
    viz_image[active_pixels_mask] = colored_active_pixels

    return viz_image


print("✅ Step 3/5: Corrected HYBRID helper function is defined.")

def process_video_with_hybrid_viz():
    # --- 1. INITIALIZATION ---
    print("🚀 Step 4/5: Starting video processing for HYBRID VISUALIZATION...")
    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)
    if not cap.isOpened(): print("❌ Error: Could not open video file."); return

    original_width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); original_height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)); fps=cap.get(cv2.CAP_PROP_FPS)
    if fps == 0: fps = 30
    new_height = int(REDUCED_WIDTH * (original_height / original_width)); new_dim = (REDUCED_WIDTH, new_height)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v'); out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, new_dim)

    # --- 2. SETUP FOR THE LOOP ---
    ret, prev_frame_orig = cap.read()
    if not ret: print("❌ Error: Could not read the first frame."); cap.release(); return
    prev_frame = cv2.resize(prev_frame_orig, new_dim, interpolation=cv2.INTER_AREA)
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    frame_count = 0

    # --- 3. MAIN PROCESSING LOOP ---
    while True:
        ret, frame_orig = cap.read()
        if not ret: break
        frame = cv2.resize(frame_orig, new_dim, interpolation=cv2.INTER_AREA)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # A. Calculate Optical Flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # B. Get Cluster Visualization using the HYBRID function
        cluster_viz_frame = get_hybrid_cluster_visualization(flow, PRE_FILTER_THRESHOLD, N_CLUSTERS, SAMPLING_FACTOR)

        # C. Blend the Visualization with the Original Frame
        alpha = 0.5 # 50% transparency
        output_frame = cv2.addWeighted(frame, 1 - alpha, cluster_viz_frame, alpha, 0)

        # D. Write the result and prepare for the next frame
        out.write(output_frame)
        prev_gray = gray
        frame_count += 1
        if frame_count % 60 == 0:
            print(f"    ... processed {frame_count} frames.")

    # --- 4. FINALIZATION ---
    print("\n🏁 Step 5/5: Processing complete.")
    cap.release()
    out.release()
    print(f"✅ Output video saved successfully to: {OUTPUT_VIDEO_PATH}")

if os.path.exists(INPUT_VIDEO_PATH):
    process_video_with_hybrid_viz()
else:
    print("Skipping processing because the input file was not found. Please check the path in Cell 2.")