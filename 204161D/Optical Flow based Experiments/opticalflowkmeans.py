# -*- coding: utf-8 -*-
"""OpticalFlowKMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Io9tv_xycm6-dX4DL0Mg-YvTY_zU0SW
"""

# ðŸ› ï¸ Install YOLOv8 and dependencies
!pip install ultralytics --quiet
!pip install opencv-python-headless --quiet
!pip install tqdm

# ðŸ“¦ Required libraries
from google.colab import drive
import cv2
import numpy as np
import pandas as pd
from ultralytics import YOLO
import matplotlib.pyplot as plt
import os
from tqdm import tqdm

# ðŸ“‚ Mount Google Drive
drive.mount('/content/drive')

# ðŸ“ Define paths
video_path = "/content/drive/MyDrive/University/Research/optical flow and glove detection/input/input1_small.mp4"
output_path = "/content/drive/MyDrive/University/Research/optical flow and glove detection/output/motion_output10.mp4"

glove_model = YOLO("/content/drive/MyDrive/University/Research/optical flow and glove detection/gloveDetectionModel.pt")

# ðŸŽžï¸ Load video
cap = cv2.VideoCapture(video_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print(f"âœ… Video loaded: {frame_count} frames at {fps:.2f} FPS")

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# --- Detect top two players ---
def get_top_two_players(frame, model):
    results = model(frame)[0]
    player_boxes = []
    for box in results.boxes:
        conf = float(box.conf)
        if conf > 0.5:  # adjust threshold if needed
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            player_boxes.append(((x1, y1, x2, y2), conf))

    # Sort by confidence
    player_boxes.sort(key=lambda x: x[1], reverse=True)
    return [box[0] for box in player_boxes[:2]]

# --- Create ROI from two bounding boxes ---
def get_center_roi(box1, box2, margin_ratio=0.2):
    x1 = min(box1[0], box2[0])
    y1 = min(box1[1], box2[1])
    x2 = max(box1[2], box2[2])
    y2 = max(box1[3], box2[3])

    width = x2 - x1
    height = y2 - y1
    margin_w = int(width * margin_ratio)
    margin_h = int(height * margin_ratio)

    x1 = max(0, x1 - margin_w)
    y1 = max(0, y1 - margin_h)
    x2 = x2 + margin_w
    y2 = y2 + margin_h

    return (x1, y1, x2, y2)

def estimate_camera_motion(prev_frame, next_frame, roi):
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)

    # Mask to ignore ROI
    mask = np.ones_like(prev_gray, dtype=np.uint8) * 255
    x1, y1, x2, y2 = roi
    mask[y1:y2, x1:x2] = 0

    features_prev = cv2.goodFeaturesToTrack(prev_gray, maxCorners=500, qualityLevel=0.01,
                                            minDistance=5, mask=mask)

    if features_prev is None or len(features_prev) < 3:
        print("âš ï¸ Not enough features found.")
        return None

    features_next, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, next_gray, features_prev, None)
    valid_prev = features_prev[status.flatten() == 1]
    valid_next = features_next[status.flatten() == 1]

    if len(valid_prev) < 3 or len(valid_next) < 3:
        print("âš ï¸ Not enough matched points.")
        return None

    transform, _ = cv2.estimateAffine2D(valid_next, valid_prev)

    return transform