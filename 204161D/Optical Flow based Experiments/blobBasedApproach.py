# -*- coding: utf-8 -*-
"""StepByStepPipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13--yQPY_FcKBeBfVl_bTNZj5lpyFXcnR
"""

from google.colab import drive
drive.mount('/content/drive')

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import os

print("Google Drive mounted and libraries imported.")

# --- Path Configuration ---
INPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/input/input1_small.mp4'
OUTPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/output/stepByStep5.mp4'

# --- Processing Configuration ---
REDUCED_WIDTH = 480

# --- Motion Detection Configuration ---
MOTION_MAGNITUDE_THRESHOLD = 2.0
ERODE_KERNEL_SIZE = (3, 3)
DILATE_KERNEL_SIZE = (8, 8)

# --- NEW: Punch Detection Configuration ---
# The minimum area (in pixels) for a motion blob to be considered.
MIN_BLOB_AREA = 150
# The maximum area (in pixels). Helps filter out huge, non-punch motions.
MAX_BLOB_AREA = 5000
# The maximum distance (in pixels) a blob can move between frames to be considered the same track.
TRACKING_MAX_DISTANCE = 50
# The speed threshold (pixels/frame) to classify a track as a high-intensity punch.
# This is the most important parameter to tune!
PUNCH_SPEED_THRESHOLD = 40
# How many frames a track can be "lost" before we delete it.
TRACK_INACTIVITY_FRAMES = 2

# --- NEW: Punch ANALYSIS Configuration ---
# The minimum number of frames a track must exist to be considered for punch analysis.
MIN_TRACK_LENGTH = 5
# The number of recent frames over which to check for sustained speed.
SUSTAINED_SPEED_FRAMES = 5
# Dot product threshold for checking directional consistency. 1.0 is a perfectly straight line.
# 0.8 is a good starting point, allowing for slight curves.
CONSISTENT_DIRECTION_THRESHOLD = 0.8


# --- Verification and Setup ---
if not os.path.exists(INPUT_VIDEO_PATH):
    print(f"❌ ERROR: Input video not found at '{INPUT_VIDEO_PATH}'")
else:
    output_dir = os.path.dirname(OUTPUT_VIDEO_PATH)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print(f"▶️ Input video: {INPUT_VIDEO_PATH}")
    print(f"◀️ Output video will be saved to: {OUTPUT_VIDEO_PATH}")
    print("✅ Configuration is set with new punch detection parameters.")

def calculate_camera_transform(prev_gray, curr_gray):
    """
    Estimates the rigid transformation (camera motion) between two frames.
    Args:
        prev_gray: The previous frame in grayscale.
        curr_gray: The current frame in grayscale.
    Returns:
        An affine transformation matrix (2x3) or None if it fails.
    """
    # Find good features to track
    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)

    if prev_pts is None:
        return np.array([[1, 0, 0], [0, 1, 0]], dtype=np.float32) # Return identity if no points

    # Calculate optical flow to track features
    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)

    # Filter for successfully tracked points
    good_new = curr_pts[status==1]
    good_old = prev_pts[status==1]

    # Estimate the rigid transformation
    transform, _ = cv2.estimateAffinePartial2D(good_old, good_new)

    if transform is None:
      return np.array([[1, 0, 0], [0, 1, 0]], dtype=np.float32) # Return identity if transform fails

    return transform


def get_motion_mask(prev_frame, curr_frame, threshold):
    """
    Calculates a binary mask highlighting pixels with significant motion.
    Args:
        prev_frame: The previous frame (grayscale).
        curr_frame: The current frame (grayscale).
        threshold: The magnitude threshold to consider a pixel as 'moving'.
    Returns:
        A binary mask (single channel, 8-bit).
    """
    # Calculate dense optical flow
    flow = cv2.calcOpticalFlowFarneback(prev_frame, curr_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])

    # Threshold the magnitude to get the motion mask
    motion_mask = (magnitude > threshold).astype(np.uint8) * 255
    return motion_mask


def clean_mask(mask, erode_kernel, dilate_kernel):
    """
    Applies morphological operations to clean up a binary mask.
    Args:
        mask: The input binary mask.
        erode_kernel: The kernel for the erosion operation.
        dilate_kernel: The kernel for the dilation operation.
    Returns:
        The cleaned binary mask.
    """
    eroded_mask = cv2.erode(mask, erode_kernel, iterations=1)
    dilated_mask = cv2.dilate(eroded_mask, dilate_kernel, iterations=1)
    return dilated_mask


print("✅ Core processing functions defined.")

def find_motion_blobs(mask):
    """
    Finds and filters contours (blobs) in a binary motion mask.
    Args:
        mask: A single-channel binary motion mask.
    Returns:
        A list of filtered blobs. Each blob is a dictionary containing its
        contour, area, and centroid.
    """
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    blobs = []
    for contour in contours:
        area = cv2.contourArea(contour)
        if MIN_BLOB_AREA < area < MAX_BLOB_AREA:
            # Calculate the centroid of the blob
            M = cv2.moments(contour)
            if M['m00'] == 0: continue
            cx = int(M['m10'] / M['m00'])
            cy = int(M['m01'] / M['m00'])
            blobs.append({'contour': contour, 'area': area, 'centroid': (cx, cy)})
    return blobs


class SimpleBlobTracker:
    """
    A simple tracker that associates blobs between frames based on proximity.
    """
    def __init__(self):
        self.next_track_id = 0
        self.tracks = []

    def update(self, blobs):
        # --- Step 1: Predict new position and decrement inactivity counter for existing tracks ---
        for track in self.tracks:
            track['inactive_for'] += 1

        # --- Step 2: Match new blobs with existing tracks ---
        unmatched_blobs = list(range(len(blobs)))
        for track in self.tracks:
            last_pos = track['history'][-1]
            best_match_dist = float('inf')
            best_match_idx = -1

            for i, blob in enumerate(blobs):
                if i not in unmatched_blobs: continue
                dist = np.linalg.norm(np.array(last_pos) - np.array(blob['centroid']))
                if dist < TRACKING_MAX_DISTANCE and dist < best_match_dist:
                    best_match_dist = dist
                    best_match_idx = i

            if best_match_idx != -1:
                # Matched: update the track
                track['history'].append(blobs[best_match_idx]['centroid'])
                track['contour'] = blobs[best_match_idx]['contour']
                track['inactive_for'] = 0 # Reset inactivity
                unmatched_blobs.remove(best_match_idx)

        # --- Step 3: Create new tracks for unmatched blobs ---
        for idx in unmatched_blobs:
            new_track = {
                'id': self.next_track_id,
                'history': [blobs[idx]['centroid']],
                'contour': blobs[idx]['contour'],
                'inactive_for': 0
            }
            self.tracks.append(new_track)
            self.next_track_id += 1

        # --- Step 4: Remove stale tracks ---
        self.tracks = [t for t in self.tracks if t['inactive_for'] < TRACK_INACTIVITY_FRAMES]

def analyze_track_for_punch(track):
    """
    Analyzes a single track to determine if it represents a punch.
    Args:
        track: A track dictionary from the tracker.
    Returns:
        True if the track is classified as a punch, False otherwise.
    """
    # A punch needs to have some history to measure speed
    if len(track['history']) < 2:
        return False

    # Calculate the speed between the last two points of the track
    last_pos = np.array(track['history'][-1])
    prev_pos = np.array(track['history'][-2])
    speed = np.linalg.norm(last_pos - prev_pos)

    if speed > PUNCH_SPEED_THRESHOLD:
        return True
    return False

print("✅ Blob and Track analysis functions defined.")

def upgraded_analyze_track_for_punch(track):
    """
    Analyzes a track with stricter rules to filter out false positives.
    Returns:
        True if the track is classified as a punch, False otherwise.
    """
    # --- Check 1: Minimum History ---
    # The track must be long enough to analyze its behavior.
    if len(track['history']) < MIN_TRACK_LENGTH:
        return False

    # --- Check 2: Sustained High Speed ---
    # We check the speed over the last few frames, not just the most recent one.
    recent_history = track['history'][-SUSTAINED_SPEED_FRAMES:]
    speeds = []
    for i in range(1, len(recent_history)):
        speed = np.linalg.norm(np.array(recent_history[i]) - np.array(recent_history[i-1]))
        speeds.append(speed)

    # If there are no calculated speeds, it can't be a punch.
    if not speeds:
        return False

    average_speed = sum(speeds) / len(speeds)
    if average_speed < PUNCH_SPEED_THRESHOLD:
        return False # Fails the primary speed check

    # --- Check 3: Consistent Direction ---
    # A punch should move in a relatively straight line.
    vectors = []
    for i in range(1, len(recent_history)):
        vec = np.array(recent_history[i]) - np.array(recent_history[i-1])
        # Normalize the vector to get only its direction
        norm = np.linalg.norm(vec)
        if norm > 0:
            vectors.append(vec / norm)

    # We need at least two vectors to compare directions
    if len(vectors) < 2:
        return False

    # Calculate the dot product between consecutive direction vectors.
    # A dot product of 1 means they point in the exact same direction.
    dot_products = []
    for i in range(1, len(vectors)):
        dot_products.append(np.dot(vectors[i], vectors[i-1]))

    average_direction_consistency = sum(dot_products) / len(dot_products)
    if average_direction_consistency < CONSISTENT_DIRECTION_THRESHOLD:
        return False # The direction is too erratic.

    # If all checks pass, it's a high-confidence punch!
    return True

print("✅ Upgraded punch analysis function defined.")

def process_video_with_punch_detection():
    """
    Main function to run the full video processing and punch detection pipeline.
    """
    # 1. --- INITIALIZATION --- (Same as before)
    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)
    if not cap.isOpened(): print("❌ Error: Could not open video."); return
    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    new_height = int(REDUCED_WIDTH * (original_height / original_width))
    new_dim = (REDUCED_WIDTH, new_height)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, new_dim)
    print(f"🚀 Starting video processing with punch detection...")

    # Initialize the blob tracker
    tracker = SimpleBlobTracker()

    # 2. --- SETUP FOR LOOP --- (Same as before)
    ret, prev_frame_orig = cap.read()
    if not ret: print("❌ Error: Could not read the first frame."); cap.release(); return
    prev_frame = cv2.resize(prev_frame_orig, new_dim, interpolation=cv2.INTER_AREA)
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    frame_count = 0

    # 3. --- MAIN PROCESSING LOOP ---
    while True:
        ret, frame_orig = cap.read()
        if not ret: break
        frame = cv2.resize(frame_orig, new_dim, interpolation=cv2.INTER_AREA)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # --- Part 1: Get Clean Motion Mask (Same as before) ---
        transform = calculate_camera_transform(prev_gray, gray)
        stabilized_gray = cv2.warpAffine(gray, transform, new_dim, flags=cv2.WARP_INVERSE_MAP)
        motion_mask_raw = get_motion_mask(prev_gray, stabilized_gray, MOTION_MAGNITUDE_THRESHOLD)
        motion_mask_cleaned = clean_mask(motion_mask_raw, np.ones(ERODE_KERNEL_SIZE), np.ones(DILATE_KERNEL_SIZE))

        # --- Part 2: Blob Detection and Tracking ---
        blobs = find_motion_blobs(motion_mask_cleaned)
        tracker.update(blobs)

        # --- Part 3: Visualization ---
        # Create a copy of the frame to draw on
        output_frame = frame.copy()

        for track in tracker.tracks:
            # Check if the current track is a punch
            # is_punch = analyze_track_for_punch(track)
            is_punch = upgraded_analyze_track_for_punch(track)

            # Set drawing color based on classification
            color = (0, 0, 255) if is_punch else (0, 255, 0) # Red for punch, Green for normal motion
            thickness = 3 if is_punch else 2

            # Draw bounding box around the blob
            x, y, w, h = cv2.boundingRect(track['contour'])
            cv2.rectangle(output_frame, (x, y), (x + w, y + h), color, thickness)

            # Draw the track ID
            cv2.putText(output_frame, f"ID: {track['id']}", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # If it's a punch, highlight it with text
            if is_punch:
                cv2.putText(output_frame, "PUNCH!", (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # --- Part 4: Write Output and Update State ---
        out.write(output_frame)
        prev_gray = gray
        frame_count += 1
        if frame_count % 60 == 0:
            print(f"    Processed {frame_count} frames...")

    # 4. --- FINALIZATION --- (Same as before)
    print("\n🏁 Processing complete.")
    cap.release()
    out.release()
    print(f"✅ Output video saved successfully to: {OUTPUT_VIDEO_PATH}")

# --- Run the main processing function ---
if os.path.exists(INPUT_VIDEO_PATH):
    # This now calls our new function
    process_video_with_punch_detection()
else:
    print("Skipping video processing because the input file was not found.")

