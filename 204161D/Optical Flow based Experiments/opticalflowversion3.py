# -*- coding: utf-8 -*-
"""OpticalFlowVersion3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jYCrc0qPMF4aQBjVJFDGGZgzR4vzKCkJ
"""

# Cell 1: Setup and Installations
from google.colab import drive
import os

# Mount your Google Drive
drive.mount('/content/drive')

# Install the Ultralytics library for YOLO
!pip install ultralytics -q

print("✅ Setup Complete")

# Cell 2: Imports and Configuration
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from google.colab import files
from sklearn.cluster import DBSCAN
from random import randint

# --- File and Model Configuration ---
# ⚠️ Update these paths
VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/input/input1.mp4'
YOLO_MODEL_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/playerIdentificationModel2.pt'
OUTPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/output/input1_5.mp4'

# --- Detection and Clustering Parameters ---
PLAYER_CONFIDENCE_THRESHOLD = 0.5
MOTION_MAGNITUDE_THRESHOLD = 5.0

# ✅ DYNAMIC: Define cluster size as a RATIO of the player's bounding box area
CLUSTER_MIN_SIZE_RATIO = 0.005  # 0.5% of the bbox area
CLUSTER_MAX_SIZE_RATIO = 0.05   # 5% of the bbox area

DBSCAN_EPS = 10
DBSCAN_MIN_SAMPLES = 5
POSITION_WEIGHT = 1.0
FLOW_WEIGHT = 3.0

# Cell 3: Model Loading
# Check if a GPU is available and use it, otherwise use CPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Load your custom-trained YOLOv8 model
model = YOLO(YOLO_MODEL_PATH)
model.to(device)

print(f"✅ Model loaded from {YOLO_MODEL_PATH}")

# Cell 4: Core Processing Functions

def detect_players(frame: np.ndarray, model: YOLO, confidence_threshold: float) -> tuple | None:
    """Detects two players in a frame using the YOLO model."""
    results = model(frame, verbose=False)
    boxes = results[0].boxes.cpu().numpy()
    confident_boxes = [box for box in boxes if box.conf[0] > confidence_threshold]

    if len(confident_boxes) >= 2:
        confident_boxes.sort(key=lambda x: x.conf[0], reverse=True)
        box1 = confident_boxes[0].xyxy[0].astype(int)
        box2 = confident_boxes[1].xyxy[0].astype(int)
        return (box1, box2)
    return None

def calculate_roi(box1: np.ndarray, box2: np.ndarray, frame_shape: tuple, margin: int = 30) -> tuple:
    """Calculates a single Region of Interest (ROI) that contains both player boxes."""
    frame_h, frame_w = frame_shape
    x_min, y_min = min(box1[0], box2[0]), min(box1[1], box2[1])
    x_max, y_max = max(box1[2], box2[2]), max(box1[3], box2[3])
    x = max(0, x_min - margin)
    y = max(0, y_min - margin)
    w = min(frame_w, x_max + margin) - x
    h = min(frame_h, y_max + margin) - y
    return (x, y, w, h)

def calculate_optical_flow(prev_gray: np.ndarray, current_gray: np.ndarray) -> np.ndarray | None:
    """Calculates dense optical flow between two grayscale frames."""
    if prev_gray.shape != current_gray.shape: return None
    return cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

def get_fast_motion_mask(flow: np.ndarray, magnitude_threshold: float) -> np.ndarray:
    """Thresholds the optical flow to get a mask of fast-moving pixels and cleans it."""
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    mask = (magnitude > magnitude_threshold).astype(np.uint8)
    kernel = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    return mask

def cluster_motion(flow: np.ndarray, mask: np.ndarray, pos_weight: float, flow_weight: float, dbs_eps: int, dbs_min_samples: int) -> dict:
    """Clusters moving pixels using their position and flow vectors."""
    pixels = np.argwhere(mask > 0)
    if len(pixels) < dbs_min_samples:
        return {}

    pixels_xy = pixels[:, ::-1]
    flow_vectors = flow[pixels[:, 0], pixels[:, 1]]
    features = np.hstack([pixels_xy * pos_weight, flow_vectors * flow_weight])

    clustering = DBSCAN(eps=dbs_eps, min_samples=dbs_min_samples).fit(features)
    labels = clustering.labels_

    clusters = {label: pixels[labels == label] for label in np.unique(labels) if label != -1}
    return clusters

def find_punch_candidate(clusters: dict, flow: np.ndarray, min_size: int, max_size: int) -> np.ndarray | None:
    """Filters clusters to find the best punch candidate based on dynamic size and speed."""
    fastest_cluster_pixels = None
    max_avg_speed = 0

    for label, pixels in clusters.items():
        # ✅ Use dynamic min/max size passed as arguments
        if not (min_size < len(pixels) < max_size):
            continue

        cluster_flow = flow[pixels[:, 0], pixels[:, 1]]
        avg_speed = np.mean(np.linalg.norm(cluster_flow, axis=1))

        if avg_speed > max_avg_speed:
            max_avg_speed = avg_speed
            fastest_cluster_pixels = pixels

    return fastest_cluster_pixels

# Cell 5: Visualization Function

def visualize_flow(frame: np.ndarray, flow: np.ndarray) -> np.ndarray:
    """Visualizes the optical flow on the frame using HSV color space."""
    hsv = np.zeros_like(frame)
    hsv[..., 1] = 255 # Max saturation
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    hsv[..., 0] = ang * 180 / np.pi / 2
    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
    bgr_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
    return cv2.addWeighted(frame, 0.8, bgr_flow, 0.5, 0)

# Cell 6: Main Processing Loop

cap = cv2.VideoCapture(VIDEO_PATH)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (frame_width, frame_height))

ret, prev_frame = cap.read()
if ret:
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
else:
    print("Error reading the first frame.")
    cap.release()

frame_count = 0
while ret:
    # ✅ Create a black canvas for the output frame for our debug view
    output_frame = np.zeros_like(prev_frame)
    current_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    player_boxes = detect_players(prev_frame, model, PLAYER_CONFIDENCE_THRESHOLD)
    status_text = "Status: Detecting Players..."

    if player_boxes:
        status_text = "Status: Players Detected"
        box1, box2 = player_boxes
        rx, ry, rw, rh = calculate_roi(box1, box2, (frame_height, frame_width))

        flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

        if flow is not None:
            motion_mask = get_fast_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD)
            clusters = cluster_motion(flow, motion_mask, POSITION_WEIGHT, FLOW_WEIGHT, DBSCAN_EPS, DBSCAN_MIN_SAMPLES)

            # --- New Visualization Logic ---
            # Create a black canvas for the ROI debug view
            debug_roi_canvas = np.zeros((rh, rw, 3), dtype=np.uint8)

            # Draw all found motion clusters with unique random colors
            for label, pixels in clusters.items():
                color = (randint(60, 255), randint(60, 255), randint(60, 255))
                # The pixel coordinates are (y, x) relative to the ROI
                for p in pixels:
                    debug_roi_canvas[p[0], p[1]] = color

            # Find the punch candidate
            punch_candidate = find_punch_candidate(clusters, flow, 0, 0) # Placeholder sizes for now

            if punch_candidate is not None:
                status_text = "Status: PUNCH CANDIDATE DETECTED!"
                # Highlight the punch candidate cluster in bright yellow
                for p in punch_candidate:
                    debug_roi_canvas[p[0], p[1]] = (0, 255, 255) # Yellow

            # Place the debug canvas onto the main output frame
            output_frame[ry:ry+rh, rx:rx+rw] = debug_roi_canvas

    # Add text annotations to the black output frame
    cv2.putText(output_frame, f"Frame: {frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

    writer.write(output_frame)

    prev_gray = current_gray
    ret, prev_frame = cap.read()
    frame_count += 1

print(f"\n✅ Processing complete. Processed {frame_count-1} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()

# Cell 5: Visualization Function

def visualize_flow(frame: np.ndarray, flow: np.ndarray) -> np.ndarray:
    """Visualizes the optical flow on the frame using HSV color space."""
    hsv = np.zeros_like(frame)
    hsv[..., 1] = 255 # Max saturation
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    hsv[..., 0] = ang * 180 / np.pi / 2
    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
    bgr_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
    return cv2.addWeighted(frame, 0.8, bgr_flow, 0.5, 0)

# Cell 6: Main Processing Loop (Updated for New Visualization)

cap = cv2.VideoCapture(VIDEO_PATH)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (frame_width, frame_height))

ret, prev_frame = cap.read()
if ret:
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
else:
    print("Error reading the first frame.")
    cap.release()

frame_count = 0
while ret:
    output_frame = np.zeros_like(prev_frame)
    current_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    player_boxes = detect_players(prev_frame, model, PLAYER_CONFIDENCE_THRESHOLD)
    status_text = "Status: Detecting Players..."

    if player_boxes:
        status_text = "Status: Players Detected"
        box1, box2 = player_boxes
        rx, ry, rw, rh = calculate_roi(box1, box2, (frame_height, frame_width))

        # Dynamic size calculation
        bbox_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
        dynamic_min_size = int(bbox_area * CLUSTER_MIN_SIZE_RATIO)
        dynamic_max_size = int(bbox_area * CLUSTER_MAX_SIZE_RATIO)

        flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

        if flow is not None:
            # Get the clean mask of fast-moving pixels
            motion_mask = get_fast_motion_mask(flow, MOTION_MAGNITUDE_THRESHOLD)

            # --- New Visualization Logic ---
            # 1. Create the colored flow visualization for the whole ROI
            flow_viz_roi = visualize_flow(prev_frame[ry:ry+rh, rx:rx+rw], flow)
            # 2. Use the motion_mask to black out everything except the fast-moving pixels
            debug_view = cv2.bitwise_and(flow_viz_roi, flow_viz_roi, mask=motion_mask)

            # Now, find clusters from that same mask
            clusters = cluster_motion(flow, motion_mask, POSITION_WEIGHT, FLOW_WEIGHT, DBSCAN_EPS, DBSCAN_MIN_SAMPLES)
            punch_candidate = find_punch_candidate(clusters, flow, dynamic_min_size, dynamic_max_size)

            if punch_candidate is not None:
                status_text = "Status: PUNCH CANDIDATE DETECTED!"
                # 3. Draw the final candidate on top of the debug view
                for p in punch_candidate:
                    debug_view[p[0], p[1]] = (0, 255, 255) # Highlight in Yellow

            # Place the final debug view onto the output frame
            output_frame[ry:ry+rh, rx:rx+rw] = debug_view

    # Add text annotations to the output frame
    cv2.putText(output_frame, f"Frame: {frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

    writer.write(output_frame)

    prev_gray = current_gray
    ret, prev_frame = cap.read()
    frame_count += 1

print(f"\n✅ Processing complete. Processed {frame_count-1} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()

"""# Step By Step"""

# Cell 2: Imports and Simplified Configuration
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from google.colab import files

# --- Configuration ---
# ⚠️ Update these paths
VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/input/input1_small.mp4'
YOLO_MODEL_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/playerIdentificationModel2.pt'
OUTPUT_VIDEO_PATH = '/content/drive/MyDrive/University/Research/optical flow and glove detection/Version 3/output/input1_small_2.mp4'

# --- Parameters ---
PLAYER_CONFIDENCE_THRESHOLD = 0.5
# ✅ New threshold in meters per second (a fast punch is ~10-15 m/s)
PUNCH_SPEED_THRESHOLD_MS = 2.0
# ✅ Assumption for an average player's height in meters
AVG_PLAYER_HEIGHT_M = 1.8

# Cell 3: Model Loading
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = YOLO(YOLO_MODEL_PATH)
model.to(device)

print(f"✅ Model loaded from {YOLO_MODEL_PATH}")

# Cell 4: Core Functions (Simplified)

def detect_players(frame: np.ndarray, model: YOLO, confidence_threshold: float) -> tuple | None:
    """Detects two players in a frame using the YOLO model."""
    results = model(frame, verbose=False)
    boxes = results[0].boxes.cpu().numpy()
    confident_boxes = [box for box in boxes if box.conf[0] > confidence_threshold]

    if len(confident_boxes) >= 2:
        confident_boxes.sort(key=lambda x: x.conf[0], reverse=True)
        box1 = confident_boxes[0].xyxy[0].astype(int)
        box2 = confident_boxes[1].xyxy[0].astype(int)
        return (box1, box2)
    return None

def calculate_roi(box1: np.ndarray, box2: np.ndarray, frame_shape: tuple, margin: int = 30) -> tuple:
    """Calculates a single Region of Interest (ROI) that contains both player boxes."""
    frame_h, frame_w = frame_shape
    x_min, y_min = min(box1[0], box2[0]), min(box1[1], box2[1])
    x_max, y_max = max(box1[2], box2[2]), max(box1[3], box2[3])
    x = max(0, x_min - margin)
    y = max(0, y_min - margin)
    w = min(frame_w, x_max + margin) - x
    h = min(frame_h, y_max + margin) - y
    return (x, y, w, h)

def calculate_optical_flow(prev_gray: np.ndarray, current_gray: np.ndarray) -> np.ndarray | None:
    """Calculates dense optical flow between two grayscale frames."""
    if prev_gray.shape != current_gray.shape: return None
    return cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

def get_motion_mask(flow: np.ndarray, magnitude_threshold: float) -> np.ndarray:
    """Thresholds the optical flow to get a binary mask of fast-moving pixels."""
    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    mask = (magnitude > magnitude_threshold).astype(np.uint8)
    return mask

# Cell 6: Main Processing Loop

cap = cv2.VideoCapture(VIDEO_PATH)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (frame_width, frame_height))

ret, prev_frame = cap.read()
if ret:
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
else:
    print("Error reading the first frame.")
    cap.release()

frame_count = 0
while ret:
    output_frame = np.zeros_like(prev_frame)
    current_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    player_boxes = detect_players(prev_frame, model, PLAYER_CONFIDENCE_THRESHOLD)
    status_text = "Status: Detecting Players..."

    if player_boxes:
        status_text = "Status: Motion Detected"
        box1, box2 = player_boxes
        rx, ry, rw, rh = calculate_roi(box1, box2, (frame_height, frame_width))

        # --- Dynamic Threshold Calculation ---
        # Get the height of the bounding box in pixels
        bbox_height_px = box1[3] - box1[1]
        # Calculate the pixels-per-meter ratio for this frame
        pixels_per_meter = bbox_height_px / AVG_PLAYER_HEIGHT_M
        # Convert the m/s threshold to a pixels/frame threshold for this frame
        dynamic_magnitude_threshold = (PUNCH_SPEED_THRESHOLD_MS * pixels_per_meter) / fps

        flow = calculate_optical_flow(prev_gray[ry:ry+rh, rx:rx+rw], current_gray[ry:ry+rh, rx:rx+rw])

        if flow is not None:
            # Use the new dynamic threshold
            motion_mask = get_motion_mask(flow, dynamic_magnitude_threshold)

            roi_original_color = prev_frame[ry:ry+rh, rx:rx+rw]
            debug_view = cv2.bitwise_and(roi_original_color, roi_original_color, mask=motion_mask)

            output_frame[ry:ry+rh, rx:rx+rw] = debug_view

    cv2.putText(output_frame, f"Frame: {frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(output_frame, status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

    writer.write(output_frame)

    prev_gray = current_gray
    ret, prev_frame = cap.read()
    frame_count += 1

print(f"\n✅ Processing complete. Processed {frame_count-1} frames. Video saved to {OUTPUT_VIDEO_PATH}")
cap.release()
writer.release()